{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Inception_BatchNorm_ADAM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBtOqttNdJLR"
      },
      "source": [
        "from tensorflow import Tensor\n",
        "from tensorflow.keras.layers import Input, Conv2D, ReLU,LeakyReLU, BatchNormalization,\\\n",
        "                                    Add, AveragePooling2D, Flatten, Dense, Dropout,ZeroPadding2D,MaxPool2D,concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.initializers import glorot_uniform\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "\n",
        "\n",
        "#Model Creation\n",
        "\n",
        "inputs = Input(shape=(32, 32, 3))\n",
        "X = Conv2D(32, (3,3), strides = (1,1),padding='same',activation=LeakyReLU())(inputs)\n",
        "X = Conv2D(32, (3,3), strides = (1,1),padding='same',activation=LeakyReLU())(X)\n",
        "X = Conv2D(32, (3,3), strides = (1,1),padding='same',activation=LeakyReLU())(X)\n",
        "X = BatchNormalization(axis = 3)(X)\n",
        "X = MaxPool2D((3,3), strides=(1,1), padding='same')(X)\n",
        "X = Conv2D(32, (3,3), strides = (1,1),padding='same',activation=LeakyReLU())(X)\n",
        "X = Conv2D(32, (3,3), strides = (1,1),padding='same',activation=LeakyReLU())(X)\n",
        "X = Conv2D(32, (3,3), strides = (1,1),padding='same',activation=LeakyReLU())(X)\n",
        "X = BatchNormalization(axis = 3)(X)\n",
        "\n",
        "#Inception1\n",
        "conv_1 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
        "conv_1 = Conv2D(32, (3,3), padding='same', activation=LeakyReLU())(conv_1)\n",
        "conv_1 = Conv2D(32, (3,3), padding='same', activation=LeakyReLU())(conv_1)\n",
        "conv_1 = BatchNormalization(axis = 3)(conv_1)\n",
        "\n",
        "conv_2 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
        "conv_2 = Conv2D(32, (3,3), padding='same', activation=LeakyReLU())(conv_2)\n",
        "conv_2 = BatchNormalization(axis = 3)(conv_2)\n",
        "\n",
        "bn = BatchNormalization(axis = 3)(X)\n",
        "conv_3 = MaxPool2D((3,3), strides=(1,1), padding='same')(bn)\n",
        "conv_3 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(conv_3)\n",
        "\n",
        "conv_4 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
        "\n",
        "X = concatenate([conv_1,conv_2,conv_3,conv_4], axis=3)\n",
        "\n",
        "\n",
        "#Inception2\n",
        "conv_1 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
        "conv_1 = Conv2D(32, (1,7), padding='same', activation=LeakyReLU())(conv_1)\n",
        "conv_1 = Conv2D(32, (7,1), padding='same', activation=LeakyReLU())(conv_1)\n",
        "conv_1 = Conv2D(32, (1,7), padding='same', activation=LeakyReLU())(conv_1)\n",
        "conv_1 = Conv2D(32, (7,1), padding='same', activation=LeakyReLU())(conv_1)\n",
        "conv_1 = BatchNormalization(axis = 3)(conv_1)\n",
        "\n",
        "conv_2 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
        "conv_2 = Conv2D(32, (1,7), padding='same', activation=LeakyReLU())(conv_2)\n",
        "conv_2 = Conv2D(32, (7,1), padding='same', activation=LeakyReLU())(conv_2)\n",
        "conv_2 = BatchNormalization(axis = 3)(conv_2)\n",
        "\n",
        "bn = BatchNormalization(axis = 3)(X)\n",
        "conv_3 = MaxPool2D((3,3), strides=(1,1), padding='same')(bn)\n",
        "conv_3 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(conv_3)\n",
        "\n",
        "conv_4 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
        "\n",
        "X = concatenate([conv_1,conv_2,conv_3,conv_4], axis=3)\n",
        "\n",
        "\n",
        "#Inception3\n",
        "conv_1 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
        "conv_1 = Conv2D(32, (1,7), padding='same', activation=LeakyReLU())(conv_1)\n",
        "conv_1 = Conv2D(32, (7,1), padding='same', activation=LeakyReLU())(conv_1)\n",
        "conv_1 = Conv2D(32, (1,7), padding='same', activation=LeakyReLU())(conv_1)\n",
        "conv_1 = Conv2D(32, (7,1), padding='same', activation=LeakyReLU())(conv_1)\n",
        "conv_1 = BatchNormalization(axis = 3)(conv_1)\n",
        "\n",
        "conv_2 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
        "conv_2 = Conv2D(32, (1,7), padding='same', activation=LeakyReLU())(conv_2)\n",
        "conv_2 = Conv2D(32, (7,1), padding='same', activation=LeakyReLU())(conv_2)\n",
        "conv_2 = BatchNormalization(axis = 3)(conv_2)\n",
        "\n",
        "bn = BatchNormalization(axis = 3)(X)\n",
        "conv_3 = MaxPool2D((3,3), strides=(1,1), padding='same')(bn)\n",
        "conv_3 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(conv_3)\n",
        "\n",
        "conv_4 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
        "\n",
        "X = concatenate([conv_1,conv_2,conv_3,conv_4], axis=3)\n",
        "\n",
        "\n",
        "#Inception4\n",
        "conv_1 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
        "conv_1 = Conv2D(32, (3,3), padding='same', activation=LeakyReLU())(conv_1)\n",
        "conv_11 = Conv2D(32, (1,3), padding='same', activation=LeakyReLU())(conv_1)\n",
        "conv_11 = BatchNormalization(axis = 3)(conv_11)\n",
        "conv_12 = Conv2D(32, (3,1), padding='same', activation=LeakyReLU())(conv_1)\n",
        "conv_12 = BatchNormalization(axis = 3)(conv_12)\n",
        "\n",
        "conv_2 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
        "conv_21 = Conv2D(32, (1,3), padding='same', activation=LeakyReLU())(conv_2)\n",
        "conv_21 = BatchNormalization(axis = 3)(conv_21)\n",
        "conv_22 = Conv2D(32, (3,1), padding='same', activation=LeakyReLU())(conv_2)\n",
        "conv_22 = BatchNormalization(axis = 3)(conv_22)\n",
        "\n",
        "bn = BatchNormalization(axis = 3)(X)\n",
        "conv_3 = MaxPool2D((3,3), strides=(1,1), padding='same')(bn)\n",
        "conv_3 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(conv_3)\n",
        "\n",
        "conv_4 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
        "\n",
        "X = concatenate([conv_11,conv_12,conv_21,conv_22,conv_3,conv_4], axis=3)\n",
        "\n",
        "\n",
        "X = Conv2D(32, 3, activation=LeakyReLU())(X)\n",
        "X = Conv2D(64, 3, activation=LeakyReLU())(X)\n",
        "X = AveragePooling2D(4)(X)\n",
        "X = Flatten()(X)\n",
        "X = Dense(512, activation=LeakyReLU())(X)\n",
        "outputs = Dense(100, activation='softmax')(X)\n",
        "\n",
        "model = Model(inputs, outputs)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UqsqYEedN4b",
        "outputId": "480c3d61-1921-4991-c9b1-2c79ceb091de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras.datasets import cifar100\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard,EarlyStopping\n",
        "\n",
        "(x_train, Y_train), (x_test, Y_test) = cifar100.load_data()\n",
        "#x_train = x_train.astype('float32') / 255\n",
        "#x_test = x_test.astype('float32') / 255\n",
        "from keras.utils import to_categorical\n",
        "y_train = to_categorical(Y_train,100)\n",
        "y_test = to_categorical(Y_test,100)\n",
        "\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "aug_data=ImageDataGenerator(\n",
        "        rotation_range=20,     #randomly rotate images in the range (20 degrees)\n",
        "        horizontal_flip=True,  #randomly flip images\n",
        "        width_shift_range=0.1, #randomly shift images horizontally (fraction of total width)\n",
        "        shear_range = 0.2,     #Shear Intensity (Shear angle in counter-clockwise direction in degrees)\n",
        "        height_shift_range=0.1,#randomly shift images vertically (fraction of total height)\n",
        "        zoom_range=0.2,        #Range for random zoom\n",
        "        brightness_range = (0.5, 1.5))   #Range for picking a brightness shift value\n",
        "aug_data.fit(x_train)\n",
        "\n",
        "adam=Adam(learning_rate=0.001,clipnorm=1,name='adam')\n",
        "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "checkpoint = ModelCheckpoint('InceptionNet_BatchNorm_Adam_20.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
        "early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=20, verbose=1, mode='auto',restore_best_weights=True)\n",
        "hist=model.fit(aug_data.flow(x_train, y_train, batch_size=128),batch_size=128, epochs=1000, verbose=1, validation_data=(x_test, y_test),callbacks=[early,checkpoint])\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 3.6189 - accuracy: 0.1559\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.22550, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 53s 137ms/step - loss: 3.6189 - accuracy: 0.1559 - val_loss: 3.2737 - val_accuracy: 0.2255\n",
            "Epoch 2/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 2.9995 - accuracy: 0.2609\n",
            "Epoch 00002: val_accuracy improved from 0.22550 to 0.31570, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 53s 135ms/step - loss: 2.9995 - accuracy: 0.2609 - val_loss: 2.7289 - val_accuracy: 0.3157\n",
            "Epoch 3/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 2.7077 - accuracy: 0.3164\n",
            "Epoch 00003: val_accuracy improved from 0.31570 to 0.32190, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 53s 135ms/step - loss: 2.7077 - accuracy: 0.3164 - val_loss: 2.7079 - val_accuracy: 0.3219\n",
            "Epoch 4/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 2.5197 - accuracy: 0.3572\n",
            "Epoch 00004: val_accuracy improved from 0.32190 to 0.39850, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 53s 135ms/step - loss: 2.5197 - accuracy: 0.3572 - val_loss: 2.3341 - val_accuracy: 0.3985\n",
            "Epoch 5/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 2.3824 - accuracy: 0.3847\n",
            "Epoch 00005: val_accuracy did not improve from 0.39850\n",
            "391/391 [==============================] - 53s 135ms/step - loss: 2.3824 - accuracy: 0.3847 - val_loss: 2.7894 - val_accuracy: 0.3423\n",
            "Epoch 6/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 2.2641 - accuracy: 0.4077\n",
            "Epoch 00006: val_accuracy improved from 0.39850 to 0.43980, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 53s 134ms/step - loss: 2.2641 - accuracy: 0.4077 - val_loss: 2.1355 - val_accuracy: 0.4398\n",
            "Epoch 7/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 2.1795 - accuracy: 0.4268\n",
            "Epoch 00007: val_accuracy did not improve from 0.43980\n",
            "391/391 [==============================] - 52s 134ms/step - loss: 2.1795 - accuracy: 0.4268 - val_loss: 2.2676 - val_accuracy: 0.4136\n",
            "Epoch 8/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 2.0975 - accuracy: 0.4452\n",
            "Epoch 00008: val_accuracy improved from 0.43980 to 0.46490, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 53s 135ms/step - loss: 2.0975 - accuracy: 0.4452 - val_loss: 2.0227 - val_accuracy: 0.4649\n",
            "Epoch 9/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 2.0402 - accuracy: 0.4568\n",
            "Epoch 00009: val_accuracy improved from 0.46490 to 0.47220, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 52s 134ms/step - loss: 2.0402 - accuracy: 0.4568 - val_loss: 2.0278 - val_accuracy: 0.4722\n",
            "Epoch 10/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.9691 - accuracy: 0.4727\n",
            "Epoch 00010: val_accuracy did not improve from 0.47220\n",
            "391/391 [==============================] - 52s 133ms/step - loss: 1.9691 - accuracy: 0.4727 - val_loss: 2.0596 - val_accuracy: 0.4719\n",
            "Epoch 11/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.9130 - accuracy: 0.4848\n",
            "Epoch 00011: val_accuracy improved from 0.47220 to 0.49810, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 52s 133ms/step - loss: 1.9130 - accuracy: 0.4848 - val_loss: 1.8522 - val_accuracy: 0.4981\n",
            "Epoch 12/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.8603 - accuracy: 0.4978\n",
            "Epoch 00012: val_accuracy improved from 0.49810 to 0.51510, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 52s 133ms/step - loss: 1.8603 - accuracy: 0.4978 - val_loss: 1.8111 - val_accuracy: 0.5151\n",
            "Epoch 13/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.8165 - accuracy: 0.5050\n",
            "Epoch 00013: val_accuracy did not improve from 0.51510\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 1.8165 - accuracy: 0.5050 - val_loss: 1.9834 - val_accuracy: 0.4851\n",
            "Epoch 14/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.7747 - accuracy: 0.5150\n",
            "Epoch 00014: val_accuracy did not improve from 0.51510\n",
            "391/391 [==============================] - 52s 132ms/step - loss: 1.7747 - accuracy: 0.5150 - val_loss: 1.9432 - val_accuracy: 0.4965\n",
            "Epoch 15/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.7409 - accuracy: 0.5223\n",
            "Epoch 00015: val_accuracy did not improve from 0.51510\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 1.7409 - accuracy: 0.5223 - val_loss: 1.8200 - val_accuracy: 0.5129\n",
            "Epoch 16/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.6964 - accuracy: 0.5322\n",
            "Epoch 00016: val_accuracy improved from 0.51510 to 0.53120, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 1.6964 - accuracy: 0.5322 - val_loss: 1.7502 - val_accuracy: 0.5312\n",
            "Epoch 17/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.6602 - accuracy: 0.5394\n",
            "Epoch 00017: val_accuracy did not improve from 0.53120\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 1.6602 - accuracy: 0.5394 - val_loss: 1.9535 - val_accuracy: 0.4925\n",
            "Epoch 18/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.6289 - accuracy: 0.5502\n",
            "Epoch 00018: val_accuracy improved from 0.53120 to 0.54040, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 1.6289 - accuracy: 0.5502 - val_loss: 1.7165 - val_accuracy: 0.5404\n",
            "Epoch 19/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.6039 - accuracy: 0.5554\n",
            "Epoch 00019: val_accuracy did not improve from 0.54040\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 1.6039 - accuracy: 0.5554 - val_loss: 1.7563 - val_accuracy: 0.5400\n",
            "Epoch 20/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.5706 - accuracy: 0.5630\n",
            "Epoch 00020: val_accuracy improved from 0.54040 to 0.54190, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 51s 132ms/step - loss: 1.5706 - accuracy: 0.5630 - val_loss: 1.7640 - val_accuracy: 0.5419\n",
            "Epoch 21/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.5401 - accuracy: 0.5712\n",
            "Epoch 00021: val_accuracy improved from 0.54190 to 0.54720, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 1.5401 - accuracy: 0.5712 - val_loss: 1.6678 - val_accuracy: 0.5472\n",
            "Epoch 22/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.5189 - accuracy: 0.5746\n",
            "Epoch 00022: val_accuracy did not improve from 0.54720\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 1.5189 - accuracy: 0.5746 - val_loss: 1.9274 - val_accuracy: 0.5097\n",
            "Epoch 23/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.5068 - accuracy: 0.5785\n",
            "Epoch 00023: val_accuracy improved from 0.54720 to 0.55140, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 1.5068 - accuracy: 0.5785 - val_loss: 1.6739 - val_accuracy: 0.5514\n",
            "Epoch 24/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.4658 - accuracy: 0.5904\n",
            "Epoch 00024: val_accuracy improved from 0.55140 to 0.56390, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 1.4658 - accuracy: 0.5904 - val_loss: 1.6302 - val_accuracy: 0.5639\n",
            "Epoch 25/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.4473 - accuracy: 0.5939\n",
            "Epoch 00025: val_accuracy improved from 0.56390 to 0.59110, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 1.4473 - accuracy: 0.5939 - val_loss: 1.4921 - val_accuracy: 0.5911\n",
            "Epoch 26/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.4270 - accuracy: 0.5962\n",
            "Epoch 00026: val_accuracy did not improve from 0.59110\n",
            "391/391 [==============================] - 51s 132ms/step - loss: 1.4270 - accuracy: 0.5962 - val_loss: 1.6490 - val_accuracy: 0.5643\n",
            "Epoch 27/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.4006 - accuracy: 0.6037\n",
            "Epoch 00027: val_accuracy did not improve from 0.59110\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 1.4006 - accuracy: 0.6037 - val_loss: 1.6989 - val_accuracy: 0.5648\n",
            "Epoch 28/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.3856 - accuracy: 0.6083\n",
            "Epoch 00028: val_accuracy did not improve from 0.59110\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 1.3856 - accuracy: 0.6083 - val_loss: 1.7935 - val_accuracy: 0.5342\n",
            "Epoch 29/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.3773 - accuracy: 0.6068\n",
            "Epoch 00029: val_accuracy did not improve from 0.59110\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 1.3773 - accuracy: 0.6068 - val_loss: 1.6933 - val_accuracy: 0.5510\n",
            "Epoch 30/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.3544 - accuracy: 0.6146\n",
            "Epoch 00030: val_accuracy did not improve from 0.59110\n",
            "391/391 [==============================] - 51s 129ms/step - loss: 1.3544 - accuracy: 0.6146 - val_loss: 1.6909 - val_accuracy: 0.5487\n",
            "Epoch 31/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.3401 - accuracy: 0.6208\n",
            "Epoch 00031: val_accuracy did not improve from 0.59110\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 1.3401 - accuracy: 0.6208 - val_loss: 1.6406 - val_accuracy: 0.5626\n",
            "Epoch 32/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.3152 - accuracy: 0.6232\n",
            "Epoch 00032: val_accuracy did not improve from 0.59110\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 1.3152 - accuracy: 0.6232 - val_loss: 1.5414 - val_accuracy: 0.5883\n",
            "Epoch 33/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.3020 - accuracy: 0.6288\n",
            "Epoch 00033: val_accuracy did not improve from 0.59110\n",
            "391/391 [==============================] - 50s 129ms/step - loss: 1.3020 - accuracy: 0.6288 - val_loss: 1.6180 - val_accuracy: 0.5768\n",
            "Epoch 34/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.2843 - accuracy: 0.6342\n",
            "Epoch 00034: val_accuracy did not improve from 0.59110\n",
            "391/391 [==============================] - 51s 129ms/step - loss: 1.2843 - accuracy: 0.6342 - val_loss: 1.6890 - val_accuracy: 0.5677\n",
            "Epoch 35/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.2642 - accuracy: 0.6367\n",
            "Epoch 00035: val_accuracy did not improve from 0.59110\n",
            "391/391 [==============================] - 51s 129ms/step - loss: 1.2642 - accuracy: 0.6367 - val_loss: 1.5553 - val_accuracy: 0.5880\n",
            "Epoch 36/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.2580 - accuracy: 0.6375\n",
            "Epoch 00036: val_accuracy did not improve from 0.59110\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 1.2580 - accuracy: 0.6375 - val_loss: 1.6246 - val_accuracy: 0.5719\n",
            "Epoch 37/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.2373 - accuracy: 0.6444\n",
            "Epoch 00037: val_accuracy did not improve from 0.59110\n",
            "391/391 [==============================] - 51s 129ms/step - loss: 1.2373 - accuracy: 0.6444 - val_loss: 1.7033 - val_accuracy: 0.5692\n",
            "Epoch 38/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.2268 - accuracy: 0.6455\n",
            "Epoch 00038: val_accuracy did not improve from 0.59110\n",
            "391/391 [==============================] - 51s 129ms/step - loss: 1.2268 - accuracy: 0.6455 - val_loss: 1.6580 - val_accuracy: 0.5810\n",
            "Epoch 39/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.2087 - accuracy: 0.6513\n",
            "Epoch 00039: val_accuracy did not improve from 0.59110\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 1.2087 - accuracy: 0.6513 - val_loss: 1.6336 - val_accuracy: 0.5795\n",
            "Epoch 40/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.2018 - accuracy: 0.6530\n",
            "Epoch 00040: val_accuracy did not improve from 0.59110\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 1.2018 - accuracy: 0.6530 - val_loss: 1.6716 - val_accuracy: 0.5727\n",
            "Epoch 41/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.1990 - accuracy: 0.6524\n",
            "Epoch 00041: val_accuracy improved from 0.59110 to 0.59540, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 52s 133ms/step - loss: 1.1990 - accuracy: 0.6524 - val_loss: 1.5267 - val_accuracy: 0.5954\n",
            "Epoch 42/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.1716 - accuracy: 0.6580\n",
            "Epoch 00042: val_accuracy did not improve from 0.59540\n",
            "391/391 [==============================] - 52s 132ms/step - loss: 1.1716 - accuracy: 0.6580 - val_loss: 1.7172 - val_accuracy: 0.5677\n",
            "Epoch 43/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.1736 - accuracy: 0.6597\n",
            "Epoch 00043: val_accuracy did not improve from 0.59540\n",
            "391/391 [==============================] - 52s 132ms/step - loss: 1.1736 - accuracy: 0.6597 - val_loss: 1.7336 - val_accuracy: 0.5714\n",
            "Epoch 44/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.1597 - accuracy: 0.6625\n",
            "Epoch 00044: val_accuracy did not improve from 0.59540\n",
            "391/391 [==============================] - 52s 132ms/step - loss: 1.1597 - accuracy: 0.6625 - val_loss: 1.6518 - val_accuracy: 0.5791\n",
            "Epoch 45/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.1521 - accuracy: 0.6655\n",
            "Epoch 00045: val_accuracy did not improve from 0.59540\n",
            "391/391 [==============================] - 52s 132ms/step - loss: 1.1521 - accuracy: 0.6655 - val_loss: 1.6651 - val_accuracy: 0.5811\n",
            "Epoch 46/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.1428 - accuracy: 0.6672\n",
            "Epoch 00046: val_accuracy did not improve from 0.59540\n",
            "391/391 [==============================] - 52s 132ms/step - loss: 1.1428 - accuracy: 0.6672 - val_loss: 1.6848 - val_accuracy: 0.5748\n",
            "Epoch 47/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.1176 - accuracy: 0.6739\n",
            "Epoch 00047: val_accuracy improved from 0.59540 to 0.59600, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 52s 132ms/step - loss: 1.1176 - accuracy: 0.6739 - val_loss: 1.5787 - val_accuracy: 0.5960\n",
            "Epoch 48/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.1119 - accuracy: 0.6743\n",
            "Epoch 00048: val_accuracy did not improve from 0.59600\n",
            "391/391 [==============================] - 52s 132ms/step - loss: 1.1119 - accuracy: 0.6743 - val_loss: 1.7454 - val_accuracy: 0.5746\n",
            "Epoch 49/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.1016 - accuracy: 0.6781\n",
            "Epoch 00049: val_accuracy did not improve from 0.59600\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 1.1016 - accuracy: 0.6781 - val_loss: 1.6387 - val_accuracy: 0.5888\n",
            "Epoch 50/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.0886 - accuracy: 0.6807\n",
            "Epoch 00050: val_accuracy did not improve from 0.59600\n",
            "391/391 [==============================] - 52s 132ms/step - loss: 1.0886 - accuracy: 0.6807 - val_loss: 1.6159 - val_accuracy: 0.5864\n",
            "Epoch 51/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.0849 - accuracy: 0.6838\n",
            "Epoch 00051: val_accuracy did not improve from 0.59600\n",
            "391/391 [==============================] - 51s 132ms/step - loss: 1.0849 - accuracy: 0.6838 - val_loss: 1.7586 - val_accuracy: 0.5706\n",
            "Epoch 52/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.0769 - accuracy: 0.6834\n",
            "Epoch 00052: val_accuracy improved from 0.59600 to 0.59850, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 1.0769 - accuracy: 0.6834 - val_loss: 1.5753 - val_accuracy: 0.5985\n",
            "Epoch 53/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.0681 - accuracy: 0.6865\n",
            "Epoch 00053: val_accuracy did not improve from 0.59850\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 1.0681 - accuracy: 0.6865 - val_loss: 1.7355 - val_accuracy: 0.5695\n",
            "Epoch 54/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.0611 - accuracy: 0.6894\n",
            "Epoch 00054: val_accuracy improved from 0.59850 to 0.59880, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 1.0611 - accuracy: 0.6894 - val_loss: 1.6011 - val_accuracy: 0.5988\n",
            "Epoch 55/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.0448 - accuracy: 0.6924\n",
            "Epoch 00055: val_accuracy improved from 0.59880 to 0.60850, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 1.0448 - accuracy: 0.6924 - val_loss: 1.5640 - val_accuracy: 0.6085\n",
            "Epoch 56/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.0354 - accuracy: 0.6926\n",
            "Epoch 00056: val_accuracy did not improve from 0.60850\n",
            "391/391 [==============================] - 52s 132ms/step - loss: 1.0354 - accuracy: 0.6926 - val_loss: 1.7460 - val_accuracy: 0.5750\n",
            "Epoch 57/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.0306 - accuracy: 0.6961\n",
            "Epoch 00057: val_accuracy did not improve from 0.60850\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 1.0306 - accuracy: 0.6961 - val_loss: 1.6662 - val_accuracy: 0.5790\n",
            "Epoch 58/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.0225 - accuracy: 0.6961\n",
            "Epoch 00058: val_accuracy did not improve from 0.60850\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 1.0225 - accuracy: 0.6961 - val_loss: 1.5328 - val_accuracy: 0.6035\n",
            "Epoch 59/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 1.0113 - accuracy: 0.7007\n",
            "Epoch 00059: val_accuracy did not improve from 0.60850\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 1.0113 - accuracy: 0.7007 - val_loss: 1.6295 - val_accuracy: 0.5990\n",
            "Epoch 60/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.9996 - accuracy: 0.7023\n",
            "Epoch 00060: val_accuracy did not improve from 0.60850\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.9996 - accuracy: 0.7023 - val_loss: 1.7389 - val_accuracy: 0.5763\n",
            "Epoch 61/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.9842 - accuracy: 0.7094\n",
            "Epoch 00061: val_accuracy improved from 0.60850 to 0.61030, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.9842 - accuracy: 0.7094 - val_loss: 1.5789 - val_accuracy: 0.6103\n",
            "Epoch 62/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.9854 - accuracy: 0.7085\n",
            "Epoch 00062: val_accuracy did not improve from 0.61030\n",
            "391/391 [==============================] - 51s 132ms/step - loss: 0.9854 - accuracy: 0.7085 - val_loss: 1.6308 - val_accuracy: 0.5967\n",
            "Epoch 63/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.9744 - accuracy: 0.7114\n",
            "Epoch 00063: val_accuracy did not improve from 0.61030\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.9744 - accuracy: 0.7114 - val_loss: 1.6579 - val_accuracy: 0.5942\n",
            "Epoch 64/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.9775 - accuracy: 0.7094\n",
            "Epoch 00064: val_accuracy did not improve from 0.61030\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 0.9775 - accuracy: 0.7094 - val_loss: 1.6333 - val_accuracy: 0.5972\n",
            "Epoch 65/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.9670 - accuracy: 0.7142\n",
            "Epoch 00065: val_accuracy did not improve from 0.61030\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 0.9670 - accuracy: 0.7142 - val_loss: 1.6047 - val_accuracy: 0.6052\n",
            "Epoch 66/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.9541 - accuracy: 0.7145\n",
            "Epoch 00066: val_accuracy did not improve from 0.61030\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 0.9541 - accuracy: 0.7145 - val_loss: 1.6007 - val_accuracy: 0.6038\n",
            "Epoch 67/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.9499 - accuracy: 0.7158\n",
            "Epoch 00067: val_accuracy did not improve from 0.61030\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 0.9499 - accuracy: 0.7158 - val_loss: 1.6293 - val_accuracy: 0.6017\n",
            "Epoch 68/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.9418 - accuracy: 0.7200\n",
            "Epoch 00068: val_accuracy improved from 0.61030 to 0.61240, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.9418 - accuracy: 0.7200 - val_loss: 1.5875 - val_accuracy: 0.6124\n",
            "Epoch 69/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.9436 - accuracy: 0.7209\n",
            "Epoch 00069: val_accuracy did not improve from 0.61240\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 0.9436 - accuracy: 0.7209 - val_loss: 1.7201 - val_accuracy: 0.5923\n",
            "Epoch 70/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.9327 - accuracy: 0.7217\n",
            "Epoch 00070: val_accuracy did not improve from 0.61240\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.9327 - accuracy: 0.7217 - val_loss: 1.6284 - val_accuracy: 0.5994\n",
            "Epoch 71/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.9244 - accuracy: 0.7243\n",
            "Epoch 00071: val_accuracy did not improve from 0.61240\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.9244 - accuracy: 0.7243 - val_loss: 1.6425 - val_accuracy: 0.6014\n",
            "Epoch 72/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.9206 - accuracy: 0.7218\n",
            "Epoch 00072: val_accuracy did not improve from 0.61240\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.9206 - accuracy: 0.7218 - val_loss: 1.7678 - val_accuracy: 0.5811\n",
            "Epoch 73/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.9102 - accuracy: 0.7254\n",
            "Epoch 00073: val_accuracy did not improve from 0.61240\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.9102 - accuracy: 0.7254 - val_loss: 1.7521 - val_accuracy: 0.5851\n",
            "Epoch 74/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.9101 - accuracy: 0.7268\n",
            "Epoch 00074: val_accuracy did not improve from 0.61240\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.9101 - accuracy: 0.7268 - val_loss: 1.6072 - val_accuracy: 0.6053\n",
            "Epoch 75/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.9046 - accuracy: 0.7299\n",
            "Epoch 00075: val_accuracy did not improve from 0.61240\n",
            "391/391 [==============================] - 52s 132ms/step - loss: 0.9046 - accuracy: 0.7299 - val_loss: 1.6144 - val_accuracy: 0.6035\n",
            "Epoch 76/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.8844 - accuracy: 0.7334\n",
            "Epoch 00076: val_accuracy did not improve from 0.61240\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.8844 - accuracy: 0.7334 - val_loss: 1.6389 - val_accuracy: 0.6046\n",
            "Epoch 77/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.8938 - accuracy: 0.7319\n",
            "Epoch 00077: val_accuracy did not improve from 0.61240\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.8938 - accuracy: 0.7319 - val_loss: 1.6383 - val_accuracy: 0.6069\n",
            "Epoch 78/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.8768 - accuracy: 0.7376\n",
            "Epoch 00078: val_accuracy did not improve from 0.61240\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.8768 - accuracy: 0.7376 - val_loss: 1.6280 - val_accuracy: 0.6048\n",
            "Epoch 79/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.8811 - accuracy: 0.7349\n",
            "Epoch 00079: val_accuracy did not improve from 0.61240\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.8811 - accuracy: 0.7349 - val_loss: 1.6567 - val_accuracy: 0.6006\n",
            "Epoch 80/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.8795 - accuracy: 0.7348\n",
            "Epoch 00080: val_accuracy improved from 0.61240 to 0.61300, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 52s 132ms/step - loss: 0.8795 - accuracy: 0.7348 - val_loss: 1.6242 - val_accuracy: 0.6130\n",
            "Epoch 81/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.8753 - accuracy: 0.7349\n",
            "Epoch 00081: val_accuracy did not improve from 0.61300\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.8753 - accuracy: 0.7349 - val_loss: 1.6260 - val_accuracy: 0.6105\n",
            "Epoch 82/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.8596 - accuracy: 0.7398\n",
            "Epoch 00082: val_accuracy did not improve from 0.61300\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.8596 - accuracy: 0.7398 - val_loss: 1.7154 - val_accuracy: 0.5920\n",
            "Epoch 83/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.8595 - accuracy: 0.7397\n",
            "Epoch 00083: val_accuracy did not improve from 0.61300\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.8595 - accuracy: 0.7397 - val_loss: 1.7280 - val_accuracy: 0.5923\n",
            "Epoch 84/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.8462 - accuracy: 0.7437\n",
            "Epoch 00084: val_accuracy did not improve from 0.61300\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.8462 - accuracy: 0.7437 - val_loss: 1.8274 - val_accuracy: 0.5871\n",
            "Epoch 85/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.8505 - accuracy: 0.7454\n",
            "Epoch 00085: val_accuracy did not improve from 0.61300\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.8505 - accuracy: 0.7454 - val_loss: 1.7463 - val_accuracy: 0.5940\n",
            "Epoch 86/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.8487 - accuracy: 0.7435\n",
            "Epoch 00086: val_accuracy did not improve from 0.61300\n",
            "391/391 [==============================] - 51s 132ms/step - loss: 0.8487 - accuracy: 0.7435 - val_loss: 1.7409 - val_accuracy: 0.5955\n",
            "Epoch 87/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.8403 - accuracy: 0.7467\n",
            "Epoch 00087: val_accuracy improved from 0.61300 to 0.62070, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.8403 - accuracy: 0.7467 - val_loss: 1.5954 - val_accuracy: 0.6207\n",
            "Epoch 88/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.8394 - accuracy: 0.7461\n",
            "Epoch 00088: val_accuracy did not improve from 0.62070\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.8394 - accuracy: 0.7461 - val_loss: 1.6595 - val_accuracy: 0.6061\n",
            "Epoch 89/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.8265 - accuracy: 0.7519\n",
            "Epoch 00089: val_accuracy did not improve from 0.62070\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 0.8265 - accuracy: 0.7519 - val_loss: 1.7883 - val_accuracy: 0.5996\n",
            "Epoch 90/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.8220 - accuracy: 0.7518\n",
            "Epoch 00090: val_accuracy did not improve from 0.62070\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.8220 - accuracy: 0.7518 - val_loss: 1.7088 - val_accuracy: 0.6030\n",
            "Epoch 91/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.8183 - accuracy: 0.7517\n",
            "Epoch 00091: val_accuracy did not improve from 0.62070\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 0.8183 - accuracy: 0.7517 - val_loss: 1.6511 - val_accuracy: 0.6094\n",
            "Epoch 92/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.8077 - accuracy: 0.7542\n",
            "Epoch 00092: val_accuracy did not improve from 0.62070\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.8077 - accuracy: 0.7542 - val_loss: 1.6113 - val_accuracy: 0.6207\n",
            "Epoch 93/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.8094 - accuracy: 0.7522\n",
            "Epoch 00093: val_accuracy did not improve from 0.62070\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 0.8094 - accuracy: 0.7522 - val_loss: 1.6313 - val_accuracy: 0.6108\n",
            "Epoch 94/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.8101 - accuracy: 0.7531\n",
            "Epoch 00094: val_accuracy improved from 0.62070 to 0.62150, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.8101 - accuracy: 0.7531 - val_loss: 1.6543 - val_accuracy: 0.6215\n",
            "Epoch 95/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.8065 - accuracy: 0.7539\n",
            "Epoch 00095: val_accuracy did not improve from 0.62150\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 0.8065 - accuracy: 0.7539 - val_loss: 1.6757 - val_accuracy: 0.6025\n",
            "Epoch 96/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7999 - accuracy: 0.7566\n",
            "Epoch 00096: val_accuracy did not improve from 0.62150\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 0.7999 - accuracy: 0.7566 - val_loss: 1.6852 - val_accuracy: 0.6083\n",
            "Epoch 97/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7904 - accuracy: 0.7582\n",
            "Epoch 00097: val_accuracy did not improve from 0.62150\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 0.7904 - accuracy: 0.7582 - val_loss: 1.8292 - val_accuracy: 0.5874\n",
            "Epoch 98/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7812 - accuracy: 0.7617\n",
            "Epoch 00098: val_accuracy did not improve from 0.62150\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 0.7812 - accuracy: 0.7617 - val_loss: 1.6919 - val_accuracy: 0.6147\n",
            "Epoch 99/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7903 - accuracy: 0.7592\n",
            "Epoch 00099: val_accuracy did not improve from 0.62150\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 0.7903 - accuracy: 0.7592 - val_loss: 1.6369 - val_accuracy: 0.6191\n",
            "Epoch 100/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7879 - accuracy: 0.7595\n",
            "Epoch 00100: val_accuracy did not improve from 0.62150\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 0.7879 - accuracy: 0.7595 - val_loss: 1.7282 - val_accuracy: 0.6010\n",
            "Epoch 101/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7753 - accuracy: 0.7620\n",
            "Epoch 00101: val_accuracy did not improve from 0.62150\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 0.7753 - accuracy: 0.7620 - val_loss: 1.8024 - val_accuracy: 0.5906\n",
            "Epoch 102/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7751 - accuracy: 0.7628\n",
            "Epoch 00102: val_accuracy did not improve from 0.62150\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 0.7751 - accuracy: 0.7628 - val_loss: 1.6501 - val_accuracy: 0.6194\n",
            "Epoch 103/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7771 - accuracy: 0.7608\n",
            "Epoch 00103: val_accuracy improved from 0.62150 to 0.62240, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.7771 - accuracy: 0.7608 - val_loss: 1.6513 - val_accuracy: 0.6224\n",
            "Epoch 104/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7725 - accuracy: 0.7652\n",
            "Epoch 00104: val_accuracy did not improve from 0.62240\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.7725 - accuracy: 0.7652 - val_loss: 1.7418 - val_accuracy: 0.6077\n",
            "Epoch 105/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7730 - accuracy: 0.7645\n",
            "Epoch 00105: val_accuracy improved from 0.62240 to 0.62450, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 52s 132ms/step - loss: 0.7730 - accuracy: 0.7645 - val_loss: 1.6016 - val_accuracy: 0.6245\n",
            "Epoch 106/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7569 - accuracy: 0.7704\n",
            "Epoch 00106: val_accuracy improved from 0.62450 to 0.62520, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.7569 - accuracy: 0.7704 - val_loss: 1.6405 - val_accuracy: 0.6252\n",
            "Epoch 107/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7482 - accuracy: 0.7703\n",
            "Epoch 00107: val_accuracy did not improve from 0.62520\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.7482 - accuracy: 0.7703 - val_loss: 1.6995 - val_accuracy: 0.6230\n",
            "Epoch 108/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7565 - accuracy: 0.7670\n",
            "Epoch 00108: val_accuracy did not improve from 0.62520\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 0.7565 - accuracy: 0.7670 - val_loss: 1.7209 - val_accuracy: 0.6198\n",
            "Epoch 109/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7360 - accuracy: 0.7734\n",
            "Epoch 00109: val_accuracy did not improve from 0.62520\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 0.7360 - accuracy: 0.7734 - val_loss: 1.7392 - val_accuracy: 0.6065\n",
            "Epoch 110/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7413 - accuracy: 0.7727\n",
            "Epoch 00110: val_accuracy improved from 0.62520 to 0.63170, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.7413 - accuracy: 0.7727 - val_loss: 1.6145 - val_accuracy: 0.6317\n",
            "Epoch 111/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7504 - accuracy: 0.7706\n",
            "Epoch 00111: val_accuracy did not improve from 0.63170\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.7504 - accuracy: 0.7706 - val_loss: 1.6924 - val_accuracy: 0.6152\n",
            "Epoch 112/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7392 - accuracy: 0.7733\n",
            "Epoch 00112: val_accuracy did not improve from 0.63170\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.7392 - accuracy: 0.7733 - val_loss: 1.7872 - val_accuracy: 0.6104\n",
            "Epoch 113/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7388 - accuracy: 0.7713\n",
            "Epoch 00113: val_accuracy did not improve from 0.63170\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.7388 - accuracy: 0.7713 - val_loss: 1.6693 - val_accuracy: 0.6168\n",
            "Epoch 114/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7251 - accuracy: 0.7780\n",
            "Epoch 00114: val_accuracy did not improve from 0.63170\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.7251 - accuracy: 0.7780 - val_loss: 1.7635 - val_accuracy: 0.6047\n",
            "Epoch 115/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7252 - accuracy: 0.7774\n",
            "Epoch 00115: val_accuracy did not improve from 0.63170\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.7252 - accuracy: 0.7774 - val_loss: 1.7164 - val_accuracy: 0.6212\n",
            "Epoch 116/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7256 - accuracy: 0.7770\n",
            "Epoch 00116: val_accuracy did not improve from 0.63170\n",
            "391/391 [==============================] - 52s 132ms/step - loss: 0.7256 - accuracy: 0.7770 - val_loss: 1.7005 - val_accuracy: 0.6173\n",
            "Epoch 117/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7144 - accuracy: 0.7814\n",
            "Epoch 00117: val_accuracy did not improve from 0.63170\n",
            "391/391 [==============================] - 52s 132ms/step - loss: 0.7144 - accuracy: 0.7814 - val_loss: 1.6990 - val_accuracy: 0.6201\n",
            "Epoch 118/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7114 - accuracy: 0.7821\n",
            "Epoch 00118: val_accuracy did not improve from 0.63170\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.7114 - accuracy: 0.7821 - val_loss: 1.7053 - val_accuracy: 0.6149\n",
            "Epoch 119/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7145 - accuracy: 0.7825\n",
            "Epoch 00119: val_accuracy did not improve from 0.63170\n",
            "391/391 [==============================] - 51s 132ms/step - loss: 0.7145 - accuracy: 0.7825 - val_loss: 1.7198 - val_accuracy: 0.6245\n",
            "Epoch 120/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7140 - accuracy: 0.7801\n",
            "Epoch 00120: val_accuracy did not improve from 0.63170\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.7140 - accuracy: 0.7801 - val_loss: 1.8382 - val_accuracy: 0.5984\n",
            "Epoch 121/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7113 - accuracy: 0.7814\n",
            "Epoch 00121: val_accuracy did not improve from 0.63170\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.7113 - accuracy: 0.7814 - val_loss: 1.7253 - val_accuracy: 0.6170\n",
            "Epoch 122/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7031 - accuracy: 0.7822\n",
            "Epoch 00122: val_accuracy did not improve from 0.63170\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.7031 - accuracy: 0.7822 - val_loss: 1.6316 - val_accuracy: 0.6211\n",
            "Epoch 123/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7066 - accuracy: 0.7833\n",
            "Epoch 00123: val_accuracy did not improve from 0.63170\n",
            "391/391 [==============================] - 52s 132ms/step - loss: 0.7066 - accuracy: 0.7833 - val_loss: 1.8219 - val_accuracy: 0.6076\n",
            "Epoch 124/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.6999 - accuracy: 0.7859\n",
            "Epoch 00124: val_accuracy did not improve from 0.63170\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.6999 - accuracy: 0.7859 - val_loss: 1.7289 - val_accuracy: 0.6144\n",
            "Epoch 125/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.7029 - accuracy: 0.7840\n",
            "Epoch 00125: val_accuracy improved from 0.63170 to 0.63230, saving model to InceptionNet_BatchNorm_Adam_20.hdf5\n",
            "391/391 [==============================] - 51s 132ms/step - loss: 0.7029 - accuracy: 0.7840 - val_loss: 1.6583 - val_accuracy: 0.6323\n",
            "Epoch 126/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.7859\n",
            "Epoch 00126: val_accuracy did not improve from 0.63230\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.6930 - accuracy: 0.7859 - val_loss: 1.7744 - val_accuracy: 0.6157\n",
            "Epoch 127/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.6924 - accuracy: 0.7849\n",
            "Epoch 00127: val_accuracy did not improve from 0.63230\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.6924 - accuracy: 0.7849 - val_loss: 1.6987 - val_accuracy: 0.6121\n",
            "Epoch 128/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.6798 - accuracy: 0.7914\n",
            "Epoch 00128: val_accuracy did not improve from 0.63230\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.6798 - accuracy: 0.7914 - val_loss: 1.7526 - val_accuracy: 0.6038\n",
            "Epoch 129/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.6882 - accuracy: 0.7874\n",
            "Epoch 00129: val_accuracy did not improve from 0.63230\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.6882 - accuracy: 0.7874 - val_loss: 1.6719 - val_accuracy: 0.6247\n",
            "Epoch 130/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.6899 - accuracy: 0.7877\n",
            "Epoch 00130: val_accuracy did not improve from 0.63230\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.6899 - accuracy: 0.7877 - val_loss: 1.7067 - val_accuracy: 0.6097\n",
            "Epoch 131/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.6822 - accuracy: 0.7871\n",
            "Epoch 00131: val_accuracy did not improve from 0.63230\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.6822 - accuracy: 0.7871 - val_loss: 1.8640 - val_accuracy: 0.5979\n",
            "Epoch 132/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.6891 - accuracy: 0.7885\n",
            "Epoch 00132: val_accuracy did not improve from 0.63230\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.6891 - accuracy: 0.7885 - val_loss: 1.7328 - val_accuracy: 0.6315\n",
            "Epoch 133/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.6797 - accuracy: 0.7898\n",
            "Epoch 00133: val_accuracy did not improve from 0.63230\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.6797 - accuracy: 0.7898 - val_loss: 1.8183 - val_accuracy: 0.6072\n",
            "Epoch 134/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.6737 - accuracy: 0.7918\n",
            "Epoch 00134: val_accuracy did not improve from 0.63230\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.6737 - accuracy: 0.7918 - val_loss: 1.8739 - val_accuracy: 0.6042\n",
            "Epoch 135/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.6785 - accuracy: 0.7898\n",
            "Epoch 00135: val_accuracy did not improve from 0.63230\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.6785 - accuracy: 0.7898 - val_loss: 1.6546 - val_accuracy: 0.6246\n",
            "Epoch 136/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.6717 - accuracy: 0.7910\n",
            "Epoch 00136: val_accuracy did not improve from 0.63230\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.6717 - accuracy: 0.7910 - val_loss: 1.7516 - val_accuracy: 0.6175\n",
            "Epoch 137/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.6739 - accuracy: 0.7920\n",
            "Epoch 00137: val_accuracy did not improve from 0.63230\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.6739 - accuracy: 0.7920 - val_loss: 1.7766 - val_accuracy: 0.6168\n",
            "Epoch 138/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.6639 - accuracy: 0.7932\n",
            "Epoch 00138: val_accuracy did not improve from 0.63230\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.6639 - accuracy: 0.7932 - val_loss: 1.7019 - val_accuracy: 0.6264\n",
            "Epoch 139/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.6631 - accuracy: 0.7940\n",
            "Epoch 00139: val_accuracy did not improve from 0.63230\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.6631 - accuracy: 0.7940 - val_loss: 1.7430 - val_accuracy: 0.6194\n",
            "Epoch 140/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.6602 - accuracy: 0.7962\n",
            "Epoch 00140: val_accuracy did not improve from 0.63230\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.6602 - accuracy: 0.7962 - val_loss: 1.8437 - val_accuracy: 0.6141\n",
            "Epoch 141/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.6559 - accuracy: 0.7980\n",
            "Epoch 00141: val_accuracy did not improve from 0.63230\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.6559 - accuracy: 0.7980 - val_loss: 2.0159 - val_accuracy: 0.5858\n",
            "Epoch 142/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.6644 - accuracy: 0.7950\n",
            "Epoch 00142: val_accuracy did not improve from 0.63230\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.6644 - accuracy: 0.7950 - val_loss: 1.7612 - val_accuracy: 0.6195\n",
            "Epoch 143/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.6550 - accuracy: 0.7960\n",
            "Epoch 00143: val_accuracy did not improve from 0.63230\n",
            "391/391 [==============================] - 51s 130ms/step - loss: 0.6550 - accuracy: 0.7960 - val_loss: 1.6949 - val_accuracy: 0.6258\n",
            "Epoch 144/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.6601 - accuracy: 0.7949\n",
            "Epoch 00144: val_accuracy did not improve from 0.63230\n",
            "391/391 [==============================] - 51s 131ms/step - loss: 0.6601 - accuracy: 0.7949 - val_loss: 1.8663 - val_accuracy: 0.6141\n",
            "Epoch 145/1000\n",
            "391/391 [==============================] - ETA: 0s - loss: 0.6494 - accuracy: 0.7994Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00145: val_accuracy did not improve from 0.63230\n",
            "391/391 [==============================] - 51s 132ms/step - loss: 0.6494 - accuracy: 0.7994 - val_loss: 1.7283 - val_accuracy: 0.6167\n",
            "Epoch 00145: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oolyL5qmdQC4",
        "outputId": "f07548a7-bf2a-42a5-88f0-05f35db2255a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "\n",
        "# Test the model\n",
        "y_true = y_test.argmax(-1)\n",
        "y_pred = model.predict(x_test).argmax(-1)\n",
        "# generate confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
        "confusion_matrix(y_true, y_pred)\n",
        "# calculate prec, recall, accuracy\n",
        "print(\"Prec: \"+ str(precision_score(y_true, y_pred, average='weighted')))\n",
        "print(\"Recall: \"+ str(recall_score(y_true, y_pred, average='weighted')))\n",
        "print(\"Accuracy: \" + str(accuracy_score(y_true, y_pred)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prec: 0.6529498300998248\n",
            "Recall: 0.6323\n",
            "Accuracy: 0.6323\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbQbvhL8p8_8",
        "outputId": "0281d2b5-6776-4ae3-ac77-e855ff16a8c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(hist.history[\"accuracy\"])\n",
        "plt.plot(hist.history['val_accuracy'])\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title(\"model accuracy\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend([\"Accuracy\",\"Validation Accuracy\",\"loss\",\"Validation Loss\"])\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqXnr7Gk2wrS"
      },
      "source": [
        "## Cell to Load Weights and Print Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Zo5y8riqF5M",
        "outputId": "9290f54d-15c6-4562-d2a0-a6f954e66bcd"
      },
      "source": [
        "from tensorflow import Tensor\n",
        "from tensorflow.keras.layers import Input, Conv2D, ReLU,LeakyReLU, BatchNormalization,\\\n",
        "                                    Add, AveragePooling2D, Flatten, Dense, Dropout,ZeroPadding2D,MaxPool2D,concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.initializers import glorot_uniform\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "\n",
        "inputs = Input(shape=(32, 32, 3))\n",
        "X = Conv2D(32, (3,3), strides = (1,1),padding='same',activation=LeakyReLU())(inputs)\n",
        "X = Conv2D(32, (3,3), strides = (1,1),padding='same',activation=LeakyReLU())(X)\n",
        "X = Conv2D(32, (3,3), strides = (1,1),padding='same',activation=LeakyReLU())(X)\n",
        "X = BatchNormalization(axis = 3)(X)\n",
        "X = MaxPool2D((3,3), strides=(1,1), padding='same')(X)\n",
        "X = Conv2D(32, (3,3), strides = (1,1),padding='same',activation=LeakyReLU())(X)\n",
        "X = Conv2D(32, (3,3), strides = (1,1),padding='same',activation=LeakyReLU())(X)\n",
        "X = Conv2D(32, (3,3), strides = (1,1),padding='same',activation=LeakyReLU())(X)\n",
        "X = BatchNormalization(axis = 3)(X)\n",
        "\n",
        "#Inception1\n",
        "conv_1 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
        "conv_1 = Conv2D(32, (3,3), padding='same', activation=LeakyReLU())(conv_1)\n",
        "conv_1 = Conv2D(32, (3,3), padding='same', activation=LeakyReLU())(conv_1)\n",
        "conv_1 = BatchNormalization(axis = 3)(conv_1)\n",
        "\n",
        "conv_2 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
        "conv_2 = Conv2D(32, (3,3), padding='same', activation=LeakyReLU())(conv_2)\n",
        "conv_2 = BatchNormalization(axis = 3)(conv_2)\n",
        "\n",
        "bn = BatchNormalization(axis = 3)(X)\n",
        "conv_3 = MaxPool2D((3,3), strides=(1,1), padding='same')(bn)\n",
        "conv_3 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(conv_3)\n",
        "\n",
        "conv_4 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
        "\n",
        "X = concatenate([conv_1,conv_2,conv_3,conv_4], axis=3)\n",
        "\n",
        "\n",
        "#Inception2\n",
        "conv_1 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
        "conv_1 = Conv2D(32, (1,7), padding='same', activation=LeakyReLU())(conv_1)\n",
        "conv_1 = Conv2D(32, (7,1), padding='same', activation=LeakyReLU())(conv_1)\n",
        "conv_1 = Conv2D(32, (1,7), padding='same', activation=LeakyReLU())(conv_1)\n",
        "conv_1 = Conv2D(32, (7,1), padding='same', activation=LeakyReLU())(conv_1)\n",
        "conv_1 = BatchNormalization(axis = 3)(conv_1)\n",
        "\n",
        "conv_2 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
        "conv_2 = Conv2D(32, (1,7), padding='same', activation=LeakyReLU())(conv_2)\n",
        "conv_2 = Conv2D(32, (7,1), padding='same', activation=LeakyReLU())(conv_2)\n",
        "conv_2 = BatchNormalization(axis = 3)(conv_2)\n",
        "\n",
        "bn = BatchNormalization(axis = 3)(X)\n",
        "conv_3 = MaxPool2D((3,3), strides=(1,1), padding='same')(bn)\n",
        "conv_3 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(conv_3)\n",
        "\n",
        "conv_4 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
        "\n",
        "X = concatenate([conv_1,conv_2,conv_3,conv_4], axis=3)\n",
        "\n",
        "\n",
        "#Inception3\n",
        "conv_1 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
        "conv_1 = Conv2D(32, (3,3), padding='same', activation=LeakyReLU())(conv_1)\n",
        "conv_11 = Conv2D(32, (1,3), padding='same', activation=LeakyReLU())(conv_1)\n",
        "conv_11 = BatchNormalization(axis = 3)(conv_11)\n",
        "conv_12 = Conv2D(32, (3,1), padding='same', activation=LeakyReLU())(conv_1)\n",
        "conv_12 = BatchNormalization(axis = 3)(conv_12)\n",
        "\n",
        "conv_2 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
        "conv_21 = Conv2D(32, (1,3), padding='same', activation=LeakyReLU())(conv_2)\n",
        "conv_21 = BatchNormalization(axis = 3)(conv_21)\n",
        "conv_22 = Conv2D(32, (3,1), padding='same', activation=LeakyReLU())(conv_2)\n",
        "conv_22 = BatchNormalization(axis = 3)(conv_22)\n",
        "\n",
        "bn = BatchNormalization(axis = 3)(X)\n",
        "conv_3 = MaxPool2D((3,3), strides=(1,1), padding='same')(bn)\n",
        "conv_3 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(conv_3)\n",
        "\n",
        "conv_4 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
        "\n",
        "X = concatenate([conv_11,conv_12,conv_21,conv_22,conv_3,conv_4], axis=3)\n",
        "\n",
        "\n",
        "X = Conv2D(32, 3, activation=LeakyReLU())(X)\n",
        "X = Conv2D(64, 3, activation=LeakyReLU())(X)\n",
        "X = AveragePooling2D(4)(X)\n",
        "X = Flatten()(X)\n",
        "X = Dense(512, activation=LeakyReLU())(X)\n",
        "outputs = Dense(100, activation='softmax')(X)\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "from tensorflow.keras.datasets import cifar100\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard,EarlyStopping\n",
        "\n",
        "(x_train, Y_train), (x_test, Y_test) = cifar100.load_data()\n",
        "#x_train = x_train.astype('float32') / 255\n",
        "#x_test = x_test.astype('float32') / 255\n",
        "from keras.utils import to_categorical\n",
        "y_train = to_categorical(Y_train,100)\n",
        "y_test = to_categorical(Y_test,100)\n",
        "\n",
        "model.load_weights('../weights/InceptionNet_BatchNorm_Adam_20.hdf5')\n",
        "\n",
        "\n",
        "# Test the model\n",
        "y_true = y_test.argmax(-1)\n",
        "y_pred = model.predict(x_test).argmax(-1)\n",
        "# generate confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
        "confusion_matrix(y_true, y_pred)\n",
        "# calculate prec, recall, accuracy\n",
        "print(\"Prec: \"+ str(precision_score(y_true, y_pred, average='weighted')))\n",
        "print(\"Recall: \"+ str(recall_score(y_true, y_pred, average='weighted')))\n",
        "print(\"Accuracy: \" + str(accuracy_score(y_true, y_pred)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Prec: 0.6202191663168878\n",
            "Recall: 0.6037\n",
            "Accuracy: 0.6037\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOauPiwS2wrV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}