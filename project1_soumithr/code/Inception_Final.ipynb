{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQ6fn0KQpE4v"
   },
   "source": [
    "##Model Without EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GKQ2W6Al7i3i"
   },
   "outputs": [],
   "source": [
    "from tensorflow import Tensor\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU,LeakyReLU, BatchNormalization,\\\n",
    "                                    Add, AveragePooling2D, Flatten, Dense, Dropout,ZeroPadding2D,MaxPool2D,concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "\n",
    "\n",
    "#Model Creation\n",
    "\n",
    "inputs = Input(shape=(32, 32, 3))\n",
    "X = Conv2D(32, (3,3), strides = (1,1),padding='same',activation=LeakyReLU())(inputs)\n",
    "X = Conv2D(32, (3,3), strides = (1,1),padding='same',activation=LeakyReLU())(X)\n",
    "X = Conv2D(32, (3,3), strides = (1,1),padding='same',activation=LeakyReLU())(X)\n",
    "X = BatchNormalization(axis = 3)(X)\n",
    "X = MaxPool2D((3,3), strides=(1,1), padding='same')(X)\n",
    "X = Conv2D(32, (3,3), strides = (1,1),padding='same',activation=LeakyReLU())(X)\n",
    "X = Conv2D(32, (3,3), strides = (1,1),padding='same',activation=LeakyReLU())(X)\n",
    "X = Conv2D(32, (3,3), strides = (1,1),padding='same',activation=LeakyReLU())(X)\n",
    "X = BatchNormalization(axis = 3)(X)\n",
    "\n",
    "#Inception1\n",
    "conv_1 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
    "conv_1 = Conv2D(32, (3,3), padding='same', activation=LeakyReLU())(conv_1)\n",
    "conv_1 = Conv2D(32, (3,3), padding='same', activation=LeakyReLU())(conv_1)\n",
    "conv_1 = BatchNormalization(axis = 3)(conv_1)\n",
    "\n",
    "conv_2 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
    "conv_2 = Conv2D(32, (3,3), padding='same', activation=LeakyReLU())(conv_2)\n",
    "conv_2 = BatchNormalization(axis = 3)(conv_2)\n",
    "\n",
    "bn = BatchNormalization(axis = 3)(X)\n",
    "conv_3 = MaxPool2D((3,3), strides=(1,1), padding='same')(bn)\n",
    "conv_3 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(conv_3)\n",
    "\n",
    "conv_4 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
    "\n",
    "X = concatenate([conv_1,conv_2,conv_3,conv_4], axis=3)\n",
    "\n",
    "\n",
    "#Inception2\n",
    "conv_1 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
    "conv_1 = Conv2D(32, (1,7), padding='same', activation=LeakyReLU())(conv_1)\n",
    "conv_1 = Conv2D(32, (7,1), padding='same', activation=LeakyReLU())(conv_1)\n",
    "conv_1 = Conv2D(32, (1,7), padding='same', activation=LeakyReLU())(conv_1)\n",
    "conv_1 = Conv2D(32, (7,1), padding='same', activation=LeakyReLU())(conv_1)\n",
    "conv_1 = BatchNormalization(axis = 3)(conv_1)\n",
    "\n",
    "conv_2 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
    "conv_2 = Conv2D(32, (1,7), padding='same', activation=LeakyReLU())(conv_2)\n",
    "conv_2 = Conv2D(32, (7,1), padding='same', activation=LeakyReLU())(conv_2)\n",
    "conv_2 = BatchNormalization(axis = 3)(conv_2)\n",
    "\n",
    "bn = BatchNormalization(axis = 3)(X)\n",
    "conv_3 = MaxPool2D((3,3), strides=(1,1), padding='same')(bn)\n",
    "conv_3 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(conv_3)\n",
    "\n",
    "conv_4 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
    "\n",
    "X = concatenate([conv_1,conv_2,conv_3,conv_4], axis=3)\n",
    "\n",
    "\n",
    "#Inception3\n",
    "conv_1 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
    "conv_1 = Conv2D(32, (3,3), padding='same', activation=LeakyReLU())(conv_1)\n",
    "conv_11 = Conv2D(32, (1,3), padding='same', activation=LeakyReLU())(conv_1)\n",
    "conv_11 = BatchNormalization(axis = 3)(conv_11)\n",
    "conv_12 = Conv2D(32, (3,1), padding='same', activation=LeakyReLU())(conv_1)\n",
    "conv_12 = BatchNormalization(axis = 3)(conv_12)\n",
    "\n",
    "conv_2 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
    "conv_21 = Conv2D(32, (1,3), padding='same', activation=LeakyReLU())(conv_2)\n",
    "conv_21 = BatchNormalization(axis = 3)(conv_21)\n",
    "conv_22 = Conv2D(32, (3,1), padding='same', activation=LeakyReLU())(conv_2)\n",
    "conv_22 = BatchNormalization(axis = 3)(conv_22)\n",
    "\n",
    "bn = BatchNormalization(axis = 3)(X)\n",
    "conv_3 = MaxPool2D((3,3), strides=(1,1), padding='same')(bn)\n",
    "conv_3 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(conv_3)\n",
    "\n",
    "conv_4 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
    "\n",
    "X = concatenate([conv_11,conv_12,conv_21,conv_22,conv_3,conv_4], axis=3)\n",
    "\n",
    "\n",
    "X = Conv2D(32, 3, activation=LeakyReLU())(X)\n",
    "X = Conv2D(64, 3, activation=LeakyReLU())(X)\n",
    "X = AveragePooling2D(4)(X)\n",
    "X = Flatten()(X)\n",
    "X = Dense(512, activation=LeakyReLU())(X)\n",
    "outputs = Dense(100, activation='softmax')(X)\n",
    "\n",
    "model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bmc9onPa7mI0",
    "outputId": "d44fd5b8-330e-49c3-9eab-112db4ff319b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.6181 - accuracy: 0.1586\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.23440, saving model to InceptionNet_BatchNorm_Adam.hdf5\n",
      "391/391 [==============================] - 60s 154ms/step - loss: 3.6181 - accuracy: 0.1586 - val_loss: 3.2295 - val_accuracy: 0.2344\n",
      "Epoch 2/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.9855 - accuracy: 0.2642\n",
      "Epoch 00002: val_accuracy improved from 0.23440 to 0.33280, saving model to InceptionNet_BatchNorm_Adam.hdf5\n",
      "391/391 [==============================] - 60s 153ms/step - loss: 2.9855 - accuracy: 0.2642 - val_loss: 2.6435 - val_accuracy: 0.3328\n",
      "Epoch 3/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.7031 - accuracy: 0.3187\n",
      "Epoch 00003: val_accuracy improved from 0.33280 to 0.34870, saving model to InceptionNet_BatchNorm_Adam.hdf5\n",
      "391/391 [==============================] - 60s 153ms/step - loss: 2.7031 - accuracy: 0.3187 - val_loss: 2.6378 - val_accuracy: 0.3487\n",
      "Epoch 4/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.5227 - accuracy: 0.3569\n",
      "Epoch 00004: val_accuracy improved from 0.34870 to 0.38890, saving model to InceptionNet_BatchNorm_Adam.hdf5\n",
      "391/391 [==============================] - 59s 152ms/step - loss: 2.5227 - accuracy: 0.3569 - val_loss: 2.4124 - val_accuracy: 0.3889\n",
      "Epoch 5/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.3713 - accuracy: 0.3870\n",
      "Epoch 00005: val_accuracy improved from 0.38890 to 0.42770, saving model to InceptionNet_BatchNorm_Adam.hdf5\n",
      "391/391 [==============================] - 60s 152ms/step - loss: 2.3713 - accuracy: 0.3870 - val_loss: 2.2142 - val_accuracy: 0.4277\n",
      "Epoch 6/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2677 - accuracy: 0.4101\n",
      "Epoch 00006: val_accuracy improved from 0.42770 to 0.45150, saving model to InceptionNet_BatchNorm_Adam.hdf5\n",
      "391/391 [==============================] - 60s 153ms/step - loss: 2.2677 - accuracy: 0.4101 - val_loss: 2.1039 - val_accuracy: 0.4515\n",
      "Epoch 7/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.1633 - accuracy: 0.4334\n",
      "Epoch 00007: val_accuracy did not improve from 0.45150\n",
      "391/391 [==============================] - 60s 153ms/step - loss: 2.1633 - accuracy: 0.4334 - val_loss: 2.0745 - val_accuracy: 0.4514\n",
      "Epoch 8/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.0944 - accuracy: 0.4440\n",
      "Epoch 00008: val_accuracy did not improve from 0.45150\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 2.0944 - accuracy: 0.4440 - val_loss: 2.2162 - val_accuracy: 0.4359\n",
      "Epoch 9/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.0132 - accuracy: 0.4613\n",
      "Epoch 00009: val_accuracy improved from 0.45150 to 0.47010, saving model to InceptionNet_BatchNorm_Adam.hdf5\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 2.0132 - accuracy: 0.4613 - val_loss: 2.0199 - val_accuracy: 0.4701\n",
      "Epoch 10/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.9690 - accuracy: 0.4724\n",
      "Epoch 00010: val_accuracy improved from 0.47010 to 0.50100, saving model to InceptionNet_BatchNorm_Adam.hdf5\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 1.9690 - accuracy: 0.4724 - val_loss: 1.8587 - val_accuracy: 0.5010\n",
      "Epoch 11/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.9017 - accuracy: 0.4863\n",
      "Epoch 00011: val_accuracy improved from 0.50100 to 0.50620, saving model to InceptionNet_BatchNorm_Adam.hdf5\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 1.9017 - accuracy: 0.4863 - val_loss: 1.8836 - val_accuracy: 0.5062\n",
      "Epoch 12/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.8506 - accuracy: 0.5004\n",
      "Epoch 00012: val_accuracy improved from 0.50620 to 0.50980, saving model to InceptionNet_BatchNorm_Adam.hdf5\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 1.8506 - accuracy: 0.5004 - val_loss: 1.8520 - val_accuracy: 0.5098\n",
      "Epoch 13/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.8046 - accuracy: 0.5094\n",
      "Epoch 00013: val_accuracy did not improve from 0.50980\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 1.8046 - accuracy: 0.5094 - val_loss: 1.9051 - val_accuracy: 0.4997\n",
      "Epoch 14/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.7725 - accuracy: 0.5188\n",
      "Epoch 00014: val_accuracy improved from 0.50980 to 0.53530, saving model to InceptionNet_BatchNorm_Adam.hdf5\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 1.7725 - accuracy: 0.5188 - val_loss: 1.7961 - val_accuracy: 0.5353\n",
      "Epoch 15/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.7259 - accuracy: 0.5276\n",
      "Epoch 00015: val_accuracy did not improve from 0.53530\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 1.7259 - accuracy: 0.5276 - val_loss: 1.9073 - val_accuracy: 0.5040\n",
      "Epoch 16/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.7032 - accuracy: 0.5324\n",
      "Epoch 00016: val_accuracy improved from 0.53530 to 0.54320, saving model to InceptionNet_BatchNorm_Adam.hdf5\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 1.7032 - accuracy: 0.5324 - val_loss: 1.7137 - val_accuracy: 0.5432\n",
      "Epoch 17/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.6561 - accuracy: 0.5436\n",
      "Epoch 00017: val_accuracy did not improve from 0.54320\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 1.6561 - accuracy: 0.5436 - val_loss: 1.8339 - val_accuracy: 0.5165\n",
      "Epoch 18/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.6271 - accuracy: 0.5497\n",
      "Epoch 00018: val_accuracy improved from 0.54320 to 0.54600, saving model to InceptionNet_BatchNorm_Adam.hdf5\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 1.6271 - accuracy: 0.5497 - val_loss: 1.6792 - val_accuracy: 0.5460\n",
      "Epoch 19/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.6014 - accuracy: 0.5576\n",
      "Epoch 00019: val_accuracy did not improve from 0.54600\n",
      "391/391 [==============================] - 58s 149ms/step - loss: 1.6014 - accuracy: 0.5576 - val_loss: 1.8864 - val_accuracy: 0.5078\n",
      "Epoch 20/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5802 - accuracy: 0.5642\n",
      "Epoch 00020: val_accuracy improved from 0.54600 to 0.55260, saving model to InceptionNet_BatchNorm_Adam.hdf5\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 1.5802 - accuracy: 0.5642 - val_loss: 1.6588 - val_accuracy: 0.5526\n",
      "Epoch 21/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5525 - accuracy: 0.5697\n",
      "Epoch 00021: val_accuracy did not improve from 0.55260\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 1.5525 - accuracy: 0.5697 - val_loss: 1.7717 - val_accuracy: 0.5324\n",
      "Epoch 22/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5244 - accuracy: 0.5763\n",
      "Epoch 00022: val_accuracy improved from 0.55260 to 0.56760, saving model to InceptionNet_BatchNorm_Adam.hdf5\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 1.5244 - accuracy: 0.5763 - val_loss: 1.6075 - val_accuracy: 0.5676\n",
      "Epoch 23/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.4959 - accuracy: 0.5823\n",
      "Epoch 00023: val_accuracy did not improve from 0.56760\n",
      "391/391 [==============================] - 58s 150ms/step - loss: 1.4959 - accuracy: 0.5823 - val_loss: 1.6199 - val_accuracy: 0.5627\n",
      "Epoch 24/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.4813 - accuracy: 0.5836\n",
      "Epoch 00024: val_accuracy did not improve from 0.56760\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 1.4813 - accuracy: 0.5836 - val_loss: 1.8328 - val_accuracy: 0.5368\n",
      "Epoch 25/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.4560 - accuracy: 0.5936\n",
      "Epoch 00025: val_accuracy did not improve from 0.56760\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 1.4560 - accuracy: 0.5936 - val_loss: 1.7638 - val_accuracy: 0.5510\n",
      "Epoch 26/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.4333 - accuracy: 0.5983\n",
      "Epoch 00026: val_accuracy did not improve from 0.56760\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 1.4333 - accuracy: 0.5983 - val_loss: 1.7333 - val_accuracy: 0.5565\n",
      "Epoch 27/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.4292 - accuracy: 0.5979\n",
      "Epoch 00027: val_accuracy did not improve from 0.56760\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 1.4292 - accuracy: 0.5979 - val_loss: 1.8056 - val_accuracy: 0.5438\n",
      "Epoch 28/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.4134 - accuracy: 0.6031\n",
      "Epoch 00028: val_accuracy did not improve from 0.56760\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 1.4134 - accuracy: 0.6031 - val_loss: 1.6795 - val_accuracy: 0.5538\n",
      "Epoch 29/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.3825 - accuracy: 0.6094\n",
      "Epoch 00029: val_accuracy improved from 0.56760 to 0.56900, saving model to InceptionNet_BatchNorm_Adam.hdf5\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 1.3825 - accuracy: 0.6094 - val_loss: 1.6375 - val_accuracy: 0.5690\n",
      "Epoch 30/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.3683 - accuracy: 0.6115\n",
      "Epoch 00030: val_accuracy did not improve from 0.56900\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 1.3683 - accuracy: 0.6115 - val_loss: 1.6858 - val_accuracy: 0.5632\n",
      "Epoch 31/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.3471 - accuracy: 0.6190\n",
      "Epoch 00031: val_accuracy improved from 0.56900 to 0.57620, saving model to InceptionNet_BatchNorm_Adam.hdf5\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 1.3471 - accuracy: 0.6190 - val_loss: 1.5954 - val_accuracy: 0.5762\n",
      "Epoch 32/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.3337 - accuracy: 0.6215\n",
      "Epoch 00032: val_accuracy improved from 0.57620 to 0.58390, saving model to InceptionNet_BatchNorm_Adam.hdf5\n",
      "391/391 [==============================] - 59s 152ms/step - loss: 1.3337 - accuracy: 0.6215 - val_loss: 1.5978 - val_accuracy: 0.5839\n",
      "Epoch 33/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.3178 - accuracy: 0.6233\n",
      "Epoch 00033: val_accuracy did not improve from 0.58390\n",
      "391/391 [==============================] - 59s 152ms/step - loss: 1.3178 - accuracy: 0.6233 - val_loss: 1.7103 - val_accuracy: 0.5576\n",
      "Epoch 34/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.3122 - accuracy: 0.6272\n",
      "Epoch 00034: val_accuracy did not improve from 0.58390\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 1.3122 - accuracy: 0.6272 - val_loss: 1.6620 - val_accuracy: 0.5680\n",
      "Epoch 35/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.2884 - accuracy: 0.6315\n",
      "Epoch 00035: val_accuracy did not improve from 0.58390\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 1.2884 - accuracy: 0.6315 - val_loss: 1.5947 - val_accuracy: 0.5706\n",
      "Epoch 36/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.2776 - accuracy: 0.6354\n",
      "Epoch 00036: val_accuracy did not improve from 0.58390\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 1.2776 - accuracy: 0.6354 - val_loss: 1.7502 - val_accuracy: 0.5576\n",
      "Epoch 37/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.2584 - accuracy: 0.6393\n",
      "Epoch 00037: val_accuracy did not improve from 0.58390\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 1.2584 - accuracy: 0.6393 - val_loss: 1.6473 - val_accuracy: 0.5783\n",
      "Epoch 38/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.2588 - accuracy: 0.6396\n",
      "Epoch 00038: val_accuracy did not improve from 0.58390\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 1.2588 - accuracy: 0.6396 - val_loss: 1.6242 - val_accuracy: 0.5756\n",
      "Epoch 39/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.2483 - accuracy: 0.6434\n",
      "Epoch 00039: val_accuracy improved from 0.58390 to 0.58820, saving model to InceptionNet_BatchNorm_Adam.hdf5\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 1.2483 - accuracy: 0.6434 - val_loss: 1.6197 - val_accuracy: 0.5882\n",
      "Epoch 40/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.2321 - accuracy: 0.6442\n",
      "Epoch 00040: val_accuracy did not improve from 0.58820\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 1.2321 - accuracy: 0.6442 - val_loss: 1.6360 - val_accuracy: 0.5717\n",
      "Epoch 41/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.2086 - accuracy: 0.6499\n",
      "Epoch 00041: val_accuracy did not improve from 0.58820\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 1.2086 - accuracy: 0.6499 - val_loss: 1.6075 - val_accuracy: 0.5822\n",
      "Epoch 42/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.1950 - accuracy: 0.6552\n",
      "Epoch 00042: val_accuracy did not improve from 0.58820\n",
      "391/391 [==============================] - 58s 149ms/step - loss: 1.1950 - accuracy: 0.6552 - val_loss: 1.6410 - val_accuracy: 0.5793\n",
      "Epoch 43/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.1972 - accuracy: 0.6561\n",
      "Epoch 00043: val_accuracy did not improve from 0.58820\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 1.1972 - accuracy: 0.6561 - val_loss: 1.6437 - val_accuracy: 0.5774\n",
      "Epoch 44/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.1778 - accuracy: 0.6578\n",
      "Epoch 00044: val_accuracy did not improve from 0.58820\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 1.1778 - accuracy: 0.6578 - val_loss: 1.5883 - val_accuracy: 0.5870\n",
      "Epoch 45/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.1646 - accuracy: 0.6614\n",
      "Epoch 00045: val_accuracy did not improve from 0.58820\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 1.1646 - accuracy: 0.6614 - val_loss: 1.7668 - val_accuracy: 0.5597\n",
      "Epoch 46/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.1619 - accuracy: 0.6630\n",
      "Epoch 00046: val_accuracy did not improve from 0.58820\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 1.1619 - accuracy: 0.6630 - val_loss: 1.5965 - val_accuracy: 0.5859\n",
      "Epoch 47/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.1533 - accuracy: 0.6657\n",
      "Epoch 00047: val_accuracy did not improve from 0.58820\n",
      "391/391 [==============================] - 58s 149ms/step - loss: 1.1533 - accuracy: 0.6657 - val_loss: 1.7710 - val_accuracy: 0.5591\n",
      "Epoch 48/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.1434 - accuracy: 0.6655\n",
      "Epoch 00048: val_accuracy improved from 0.58820 to 0.58870, saving model to InceptionNet_BatchNorm_Adam.hdf5\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 1.1434 - accuracy: 0.6655 - val_loss: 1.5935 - val_accuracy: 0.5887\n",
      "Epoch 49/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.1333 - accuracy: 0.6688\n",
      "Epoch 00049: val_accuracy did not improve from 0.58870\n",
      "391/391 [==============================] - 58s 149ms/step - loss: 1.1333 - accuracy: 0.6688 - val_loss: 1.6561 - val_accuracy: 0.5833\n",
      "Epoch 50/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.1259 - accuracy: 0.6722\n",
      "Epoch 00050: val_accuracy did not improve from 0.58870\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 1.1259 - accuracy: 0.6722 - val_loss: 1.6797 - val_accuracy: 0.5767\n",
      "Epoch 51/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.1089 - accuracy: 0.6760\n",
      "Epoch 00051: val_accuracy did not improve from 0.58870\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 1.1089 - accuracy: 0.6760 - val_loss: 1.7071 - val_accuracy: 0.5795\n",
      "Epoch 52/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0947 - accuracy: 0.6810\n",
      "Epoch 00052: val_accuracy improved from 0.58870 to 0.59330, saving model to InceptionNet_BatchNorm_Adam.hdf5\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 1.0947 - accuracy: 0.6810 - val_loss: 1.6236 - val_accuracy: 0.5933\n",
      "Epoch 53/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0904 - accuracy: 0.6790\n",
      "Epoch 00053: val_accuracy did not improve from 0.59330\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 1.0904 - accuracy: 0.6790 - val_loss: 1.7521 - val_accuracy: 0.5712\n",
      "Epoch 54/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0800 - accuracy: 0.6831\n",
      "Epoch 00054: val_accuracy did not improve from 0.59330\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 1.0800 - accuracy: 0.6831 - val_loss: 1.6136 - val_accuracy: 0.5913\n",
      "Epoch 55/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0752 - accuracy: 0.6862\n",
      "Epoch 00055: val_accuracy did not improve from 0.59330\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 1.0752 - accuracy: 0.6862 - val_loss: 1.6587 - val_accuracy: 0.5803\n",
      "Epoch 56/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0717 - accuracy: 0.6844\n",
      "Epoch 00056: val_accuracy improved from 0.59330 to 0.60850, saving model to InceptionNet_BatchNorm_Adam.hdf5\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 1.0717 - accuracy: 0.6844 - val_loss: 1.5188 - val_accuracy: 0.6085\n",
      "Epoch 57/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0623 - accuracy: 0.6878\n",
      "Epoch 00057: val_accuracy did not improve from 0.60850\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 1.0623 - accuracy: 0.6878 - val_loss: 1.6397 - val_accuracy: 0.5840\n",
      "Epoch 58/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0512 - accuracy: 0.6921\n",
      "Epoch 00058: val_accuracy did not improve from 0.60850\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 1.0512 - accuracy: 0.6921 - val_loss: 1.5594 - val_accuracy: 0.6002\n",
      "Epoch 59/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0460 - accuracy: 0.6924\n",
      "Epoch 00059: val_accuracy did not improve from 0.60850\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 1.0460 - accuracy: 0.6924 - val_loss: 1.6420 - val_accuracy: 0.5869\n",
      "Epoch 60/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0322 - accuracy: 0.6931\n",
      "Epoch 00060: val_accuracy did not improve from 0.60850\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 1.0322 - accuracy: 0.6931 - val_loss: 1.6745 - val_accuracy: 0.5848\n",
      "Epoch 61/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0245 - accuracy: 0.6970\n",
      "Epoch 00061: val_accuracy did not improve from 0.60850\n",
      "391/391 [==============================] - 58s 149ms/step - loss: 1.0245 - accuracy: 0.6970 - val_loss: 1.6329 - val_accuracy: 0.5886\n",
      "Epoch 62/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0263 - accuracy: 0.6989\n",
      "Epoch 00062: val_accuracy did not improve from 0.60850\n",
      "391/391 [==============================] - 58s 149ms/step - loss: 1.0263 - accuracy: 0.6989 - val_loss: 1.6656 - val_accuracy: 0.5924\n",
      "Epoch 63/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0139 - accuracy: 0.6997\n",
      "Epoch 00063: val_accuracy did not improve from 0.60850\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 1.0139 - accuracy: 0.6997 - val_loss: 1.6572 - val_accuracy: 0.5962\n",
      "Epoch 64/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0116 - accuracy: 0.7016\n",
      "Epoch 00064: val_accuracy improved from 0.60850 to 0.60860, saving model to InceptionNet_BatchNorm_Adam.hdf5\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 1.0116 - accuracy: 0.7016 - val_loss: 1.5961 - val_accuracy: 0.6086\n",
      "Epoch 65/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9998 - accuracy: 0.7034\n",
      "Epoch 00065: val_accuracy did not improve from 0.60860\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 0.9998 - accuracy: 0.7034 - val_loss: 1.6071 - val_accuracy: 0.6020\n",
      "Epoch 66/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9948 - accuracy: 0.7055\n",
      "Epoch 00066: val_accuracy did not improve from 0.60860\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 0.9948 - accuracy: 0.7055 - val_loss: 1.5957 - val_accuracy: 0.6051\n",
      "Epoch 67/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9964 - accuracy: 0.7039\n",
      "Epoch 00067: val_accuracy did not improve from 0.60860\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 0.9964 - accuracy: 0.7039 - val_loss: 1.7175 - val_accuracy: 0.5761\n",
      "Epoch 68/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9730 - accuracy: 0.7119\n",
      "Epoch 00068: val_accuracy did not improve from 0.60860\n",
      "391/391 [==============================] - 58s 147ms/step - loss: 0.9730 - accuracy: 0.7119 - val_loss: 1.6387 - val_accuracy: 0.6014\n",
      "Epoch 69/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9762 - accuracy: 0.7111\n",
      "Epoch 00069: val_accuracy did not improve from 0.60860\n",
      "391/391 [==============================] - 57s 147ms/step - loss: 0.9762 - accuracy: 0.7111 - val_loss: 1.5893 - val_accuracy: 0.6067\n",
      "Epoch 70/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9556 - accuracy: 0.7155\n",
      "Epoch 00070: val_accuracy did not improve from 0.60860\n",
      "391/391 [==============================] - 58s 148ms/step - loss: 0.9556 - accuracy: 0.7155 - val_loss: 1.5789 - val_accuracy: 0.6057\n",
      "Epoch 71/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9580 - accuracy: 0.7160\n",
      "Epoch 00071: val_accuracy did not improve from 0.60860\n",
      "391/391 [==============================] - 58s 147ms/step - loss: 0.9580 - accuracy: 0.7160 - val_loss: 1.7583 - val_accuracy: 0.5848\n",
      "Epoch 72/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9662 - accuracy: 0.7136\n",
      "Epoch 00072: val_accuracy did not improve from 0.60860\n",
      "391/391 [==============================] - 58s 148ms/step - loss: 0.9662 - accuracy: 0.7136 - val_loss: 1.6152 - val_accuracy: 0.5971\n",
      "Epoch 73/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9481 - accuracy: 0.7158\n",
      "Epoch 00073: val_accuracy did not improve from 0.60860\n",
      "391/391 [==============================] - 58s 147ms/step - loss: 0.9481 - accuracy: 0.7158 - val_loss: 1.7879 - val_accuracy: 0.5875\n",
      "Epoch 74/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9441 - accuracy: 0.7163\n",
      "Epoch 00074: val_accuracy did not improve from 0.60860\n",
      "391/391 [==============================] - 58s 148ms/step - loss: 0.9441 - accuracy: 0.7163 - val_loss: 1.5983 - val_accuracy: 0.5966\n",
      "Epoch 75/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9292 - accuracy: 0.7218\n",
      "Epoch 00075: val_accuracy improved from 0.60860 to 0.61730, saving model to InceptionNet_BatchNorm_Adam.hdf5\n",
      "391/391 [==============================] - 58s 148ms/step - loss: 0.9292 - accuracy: 0.7218 - val_loss: 1.5499 - val_accuracy: 0.6173\n",
      "Epoch 76/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9251 - accuracy: 0.7224\n",
      "Epoch 00076: val_accuracy did not improve from 0.61730\n",
      "391/391 [==============================] - 58s 147ms/step - loss: 0.9251 - accuracy: 0.7224 - val_loss: 1.7329 - val_accuracy: 0.5920\n",
      "Epoch 77/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9283 - accuracy: 0.7235\n",
      "Epoch 00077: val_accuracy did not improve from 0.61730\n",
      "391/391 [==============================] - 57s 147ms/step - loss: 0.9283 - accuracy: 0.7235 - val_loss: 1.7303 - val_accuracy: 0.5869\n",
      "Epoch 78/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9295 - accuracy: 0.7218\n",
      "Epoch 00078: val_accuracy did not improve from 0.61730\n",
      "391/391 [==============================] - 58s 147ms/step - loss: 0.9295 - accuracy: 0.7218 - val_loss: 1.6981 - val_accuracy: 0.5883\n",
      "Epoch 79/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9143 - accuracy: 0.7259\n",
      "Epoch 00079: val_accuracy did not improve from 0.61730\n",
      "391/391 [==============================] - 58s 148ms/step - loss: 0.9143 - accuracy: 0.7259 - val_loss: 1.6454 - val_accuracy: 0.6075\n",
      "Epoch 80/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9095 - accuracy: 0.7272\n",
      "Epoch 00080: val_accuracy did not improve from 0.61730\n",
      "391/391 [==============================] - 58s 149ms/step - loss: 0.9095 - accuracy: 0.7272 - val_loss: 1.6772 - val_accuracy: 0.5988\n",
      "Epoch 81/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9100 - accuracy: 0.7290\n",
      "Epoch 00081: val_accuracy did not improve from 0.61730\n",
      "391/391 [==============================] - 58s 149ms/step - loss: 0.9100 - accuracy: 0.7290 - val_loss: 1.6181 - val_accuracy: 0.6121\n",
      "Epoch 82/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9004 - accuracy: 0.7288\n",
      "Epoch 00082: val_accuracy did not improve from 0.61730\n",
      "391/391 [==============================] - 58s 147ms/step - loss: 0.9004 - accuracy: 0.7288 - val_loss: 1.6498 - val_accuracy: 0.6063\n",
      "Epoch 83/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8879 - accuracy: 0.7353\n",
      "Epoch 00083: val_accuracy did not improve from 0.61730\n",
      "391/391 [==============================] - 57s 147ms/step - loss: 0.8879 - accuracy: 0.7353 - val_loss: 1.6128 - val_accuracy: 0.6128\n",
      "Epoch 84/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8972 - accuracy: 0.7306\n",
      "Epoch 00084: val_accuracy did not improve from 0.61730\n",
      "391/391 [==============================] - 56s 144ms/step - loss: 0.8972 - accuracy: 0.7306 - val_loss: 1.7403 - val_accuracy: 0.5880\n",
      "Epoch 85/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8840 - accuracy: 0.7334\n",
      "Epoch 00085: val_accuracy did not improve from 0.61730\n",
      "391/391 [==============================] - 58s 148ms/step - loss: 0.8840 - accuracy: 0.7334 - val_loss: 1.7164 - val_accuracy: 0.5969\n",
      "Epoch 86/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8815 - accuracy: 0.7343\n",
      "Epoch 00086: val_accuracy did not improve from 0.61730\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 0.8815 - accuracy: 0.7343 - val_loss: 1.5732 - val_accuracy: 0.6059\n",
      "Epoch 87/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8772 - accuracy: 0.7361\n",
      "Epoch 00087: val_accuracy did not improve from 0.61730\n",
      "391/391 [==============================] - 57s 146ms/step - loss: 0.8772 - accuracy: 0.7361 - val_loss: 1.5762 - val_accuracy: 0.6108\n",
      "Epoch 88/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8742 - accuracy: 0.7357\n",
      "Epoch 00088: val_accuracy did not improve from 0.61730\n",
      "391/391 [==============================] - 57s 146ms/step - loss: 0.8742 - accuracy: 0.7357 - val_loss: 1.6972 - val_accuracy: 0.5956\n",
      "Epoch 89/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8653 - accuracy: 0.7387\n",
      "Epoch 00089: val_accuracy did not improve from 0.61730\n",
      "391/391 [==============================] - 58s 147ms/step - loss: 0.8653 - accuracy: 0.7387 - val_loss: 1.7363 - val_accuracy: 0.5891\n",
      "Epoch 90/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8577 - accuracy: 0.7420\n",
      "Epoch 00090: val_accuracy did not improve from 0.61730\n",
      "391/391 [==============================] - 57s 147ms/step - loss: 0.8577 - accuracy: 0.7420 - val_loss: 1.7048 - val_accuracy: 0.6062\n",
      "Epoch 91/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8644 - accuracy: 0.7418\n",
      "Epoch 00091: val_accuracy did not improve from 0.61730\n",
      "391/391 [==============================] - 58s 147ms/step - loss: 0.8644 - accuracy: 0.7418 - val_loss: 1.7221 - val_accuracy: 0.6068\n",
      "Epoch 92/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8504 - accuracy: 0.7439\n",
      "Epoch 00092: val_accuracy did not improve from 0.61730\n",
      "391/391 [==============================] - 57s 146ms/step - loss: 0.8504 - accuracy: 0.7439 - val_loss: 1.6650 - val_accuracy: 0.6018\n",
      "Epoch 93/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8509 - accuracy: 0.7446\n",
      "Epoch 00093: val_accuracy improved from 0.61730 to 0.62000, saving model to InceptionNet_BatchNorm_Adam.hdf5\n",
      "391/391 [==============================] - 58s 148ms/step - loss: 0.8509 - accuracy: 0.7446 - val_loss: 1.6461 - val_accuracy: 0.6200\n",
      "Epoch 94/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8323 - accuracy: 0.7504\n",
      "Epoch 00094: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 58s 148ms/step - loss: 0.8323 - accuracy: 0.7504 - val_loss: 1.7055 - val_accuracy: 0.5980\n",
      "Epoch 95/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8476 - accuracy: 0.7437\n",
      "Epoch 00095: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 58s 148ms/step - loss: 0.8476 - accuracy: 0.7437 - val_loss: 1.6159 - val_accuracy: 0.6114\n",
      "Epoch 96/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8374 - accuracy: 0.7476\n",
      "Epoch 00096: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 58s 148ms/step - loss: 0.8374 - accuracy: 0.7476 - val_loss: 1.7512 - val_accuracy: 0.5954\n",
      "Epoch 97/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8259 - accuracy: 0.7497\n",
      "Epoch 00097: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 58s 149ms/step - loss: 0.8259 - accuracy: 0.7497 - val_loss: 1.7028 - val_accuracy: 0.6071\n",
      "Epoch 98/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8329 - accuracy: 0.7488\n",
      "Epoch 00098: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 57s 147ms/step - loss: 0.8329 - accuracy: 0.7488 - val_loss: 1.7668 - val_accuracy: 0.5949\n",
      "Epoch 99/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8188 - accuracy: 0.7521\n",
      "Epoch 00099: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 57s 147ms/step - loss: 0.8188 - accuracy: 0.7521 - val_loss: 1.8250 - val_accuracy: 0.5866\n",
      "Epoch 100/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8222 - accuracy: 0.7513\n",
      "Epoch 00100: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 58s 148ms/step - loss: 0.8222 - accuracy: 0.7513 - val_loss: 1.7302 - val_accuracy: 0.5953\n",
      "Epoch 101/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8140 - accuracy: 0.7534\n",
      "Epoch 00101: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 57s 146ms/step - loss: 0.8140 - accuracy: 0.7534 - val_loss: 1.7090 - val_accuracy: 0.6006\n",
      "Epoch 102/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8184 - accuracy: 0.7512\n",
      "Epoch 00102: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 58s 147ms/step - loss: 0.8184 - accuracy: 0.7512 - val_loss: 1.8203 - val_accuracy: 0.5945\n",
      "Epoch 103/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8140 - accuracy: 0.7537\n",
      "Epoch 00103: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 58s 147ms/step - loss: 0.8140 - accuracy: 0.7537 - val_loss: 1.6914 - val_accuracy: 0.6021\n",
      "Epoch 104/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8131 - accuracy: 0.7545\n",
      "Epoch 00104: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 57s 146ms/step - loss: 0.8131 - accuracy: 0.7545 - val_loss: 1.6409 - val_accuracy: 0.6034\n",
      "Epoch 105/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7959 - accuracy: 0.7582\n",
      "Epoch 00105: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 58s 147ms/step - loss: 0.7959 - accuracy: 0.7582 - val_loss: 1.8522 - val_accuracy: 0.5914\n",
      "Epoch 106/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7917 - accuracy: 0.7581\n",
      "Epoch 00106: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 57s 147ms/step - loss: 0.7917 - accuracy: 0.7581 - val_loss: 1.6152 - val_accuracy: 0.6096\n",
      "Epoch 107/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8001 - accuracy: 0.7559\n",
      "Epoch 00107: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 57s 147ms/step - loss: 0.8001 - accuracy: 0.7559 - val_loss: 1.6397 - val_accuracy: 0.6199\n",
      "Epoch 108/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7841 - accuracy: 0.7616\n",
      "Epoch 00108: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 57s 147ms/step - loss: 0.7841 - accuracy: 0.7616 - val_loss: 1.7013 - val_accuracy: 0.6046\n",
      "Epoch 109/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7958 - accuracy: 0.7590\n",
      "Epoch 00109: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 57s 146ms/step - loss: 0.7958 - accuracy: 0.7590 - val_loss: 1.6917 - val_accuracy: 0.6074\n",
      "Epoch 110/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7854 - accuracy: 0.7609\n",
      "Epoch 00110: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 57s 146ms/step - loss: 0.7854 - accuracy: 0.7609 - val_loss: 1.8237 - val_accuracy: 0.5922\n",
      "Epoch 111/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7823 - accuracy: 0.7615\n",
      "Epoch 00111: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 58s 147ms/step - loss: 0.7823 - accuracy: 0.7615 - val_loss: 1.7360 - val_accuracy: 0.6048\n",
      "Epoch 112/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7838 - accuracy: 0.7622\n",
      "Epoch 00112: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 56s 144ms/step - loss: 0.7838 - accuracy: 0.7622 - val_loss: 1.7080 - val_accuracy: 0.6030\n",
      "Epoch 113/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7811 - accuracy: 0.7618\n",
      "Epoch 00113: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 58s 148ms/step - loss: 0.7811 - accuracy: 0.7618 - val_loss: 1.7754 - val_accuracy: 0.6059\n",
      "Epoch 114/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7729 - accuracy: 0.7650\n",
      "Epoch 00114: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 0.7729 - accuracy: 0.7650 - val_loss: 1.7432 - val_accuracy: 0.6063\n",
      "Epoch 115/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7633 - accuracy: 0.7666\n",
      "Epoch 00115: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 0.7633 - accuracy: 0.7666 - val_loss: 1.9151 - val_accuracy: 0.5855\n",
      "Epoch 116/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7632 - accuracy: 0.7663\n",
      "Epoch 00116: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 152ms/step - loss: 0.7632 - accuracy: 0.7663 - val_loss: 1.8640 - val_accuracy: 0.5820\n",
      "Epoch 117/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7638 - accuracy: 0.7636\n",
      "Epoch 00117: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 60s 153ms/step - loss: 0.7638 - accuracy: 0.7636 - val_loss: 1.8329 - val_accuracy: 0.5948\n",
      "Epoch 118/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7623 - accuracy: 0.7665\n",
      "Epoch 00118: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 57s 147ms/step - loss: 0.7623 - accuracy: 0.7665 - val_loss: 1.7011 - val_accuracy: 0.6031\n",
      "Epoch 119/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7611 - accuracy: 0.7685\n",
      "Epoch 00119: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 57s 145ms/step - loss: 0.7611 - accuracy: 0.7685 - val_loss: 1.6702 - val_accuracy: 0.6143\n",
      "Epoch 120/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7492 - accuracy: 0.7704\n",
      "Epoch 00120: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 0.7492 - accuracy: 0.7704 - val_loss: 1.8062 - val_accuracy: 0.6076\n",
      "Epoch 121/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7435 - accuracy: 0.7738\n",
      "Epoch 00121: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 0.7435 - accuracy: 0.7738 - val_loss: 1.7650 - val_accuracy: 0.6000\n",
      "Epoch 122/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7425 - accuracy: 0.7736\n",
      "Epoch 00122: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 58s 149ms/step - loss: 0.7425 - accuracy: 0.7736 - val_loss: 1.7953 - val_accuracy: 0.6062\n",
      "Epoch 123/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7526 - accuracy: 0.7713\n",
      "Epoch 00123: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 58s 149ms/step - loss: 0.7526 - accuracy: 0.7713 - val_loss: 1.8686 - val_accuracy: 0.5926\n",
      "Epoch 124/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7450 - accuracy: 0.7722\n",
      "Epoch 00124: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 0.7450 - accuracy: 0.7722 - val_loss: 1.8244 - val_accuracy: 0.5979\n",
      "Epoch 125/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7417 - accuracy: 0.7740\n",
      "Epoch 00125: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 0.7417 - accuracy: 0.7740 - val_loss: 1.7596 - val_accuracy: 0.6095\n",
      "Epoch 126/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7350 - accuracy: 0.7751\n",
      "Epoch 00126: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 0.7350 - accuracy: 0.7751 - val_loss: 1.7795 - val_accuracy: 0.5990\n",
      "Epoch 127/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7290 - accuracy: 0.7765\n",
      "Epoch 00127: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 0.7290 - accuracy: 0.7765 - val_loss: 1.8260 - val_accuracy: 0.6005\n",
      "Epoch 128/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7297 - accuracy: 0.7765\n",
      "Epoch 00128: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 0.7297 - accuracy: 0.7765 - val_loss: 1.8181 - val_accuracy: 0.5976\n",
      "Epoch 129/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7306 - accuracy: 0.7757\n",
      "Epoch 00129: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 152ms/step - loss: 0.7306 - accuracy: 0.7757 - val_loss: 1.7724 - val_accuracy: 0.5978\n",
      "Epoch 130/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7292 - accuracy: 0.7752\n",
      "Epoch 00130: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 0.7292 - accuracy: 0.7752 - val_loss: 1.8212 - val_accuracy: 0.5942\n",
      "Epoch 131/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7275 - accuracy: 0.7759\n",
      "Epoch 00131: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 0.7275 - accuracy: 0.7759 - val_loss: 1.8635 - val_accuracy: 0.5998\n",
      "Epoch 132/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7153 - accuracy: 0.7801\n",
      "Epoch 00132: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 0.7153 - accuracy: 0.7801 - val_loss: 1.9077 - val_accuracy: 0.5830\n",
      "Epoch 133/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7134 - accuracy: 0.7819\n",
      "Epoch 00133: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 0.7134 - accuracy: 0.7819 - val_loss: 1.7161 - val_accuracy: 0.6157\n",
      "Epoch 134/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7166 - accuracy: 0.7799\n",
      "Epoch 00134: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 0.7166 - accuracy: 0.7799 - val_loss: 1.8348 - val_accuracy: 0.6043\n",
      "Epoch 135/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7126 - accuracy: 0.7804\n",
      "Epoch 00135: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 0.7126 - accuracy: 0.7804 - val_loss: 1.8222 - val_accuracy: 0.5989\n",
      "Epoch 136/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7146 - accuracy: 0.7809\n",
      "Epoch 00136: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 0.7146 - accuracy: 0.7809 - val_loss: 1.9290 - val_accuracy: 0.5923\n",
      "Epoch 137/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7092 - accuracy: 0.7833\n",
      "Epoch 00137: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 0.7092 - accuracy: 0.7833 - val_loss: 1.7023 - val_accuracy: 0.6123\n",
      "Epoch 138/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7068 - accuracy: 0.7811\n",
      "Epoch 00138: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 57s 147ms/step - loss: 0.7068 - accuracy: 0.7811 - val_loss: 1.6871 - val_accuracy: 0.6179\n",
      "Epoch 139/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6991 - accuracy: 0.7823\n",
      "Epoch 00139: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 58s 149ms/step - loss: 0.6991 - accuracy: 0.7823 - val_loss: 1.7785 - val_accuracy: 0.6002\n",
      "Epoch 140/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7068 - accuracy: 0.7835\n",
      "Epoch 00140: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 60s 153ms/step - loss: 0.7068 - accuracy: 0.7835 - val_loss: 1.7049 - val_accuracy: 0.6171\n",
      "Epoch 141/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6967 - accuracy: 0.7858\n",
      "Epoch 00141: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 0.6967 - accuracy: 0.7858 - val_loss: 1.8990 - val_accuracy: 0.6023\n",
      "Epoch 142/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7031 - accuracy: 0.7830\n",
      "Epoch 00142: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 0.7031 - accuracy: 0.7830 - val_loss: 1.8317 - val_accuracy: 0.6080\n",
      "Epoch 143/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6912 - accuracy: 0.7867\n",
      "Epoch 00143: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 0.6912 - accuracy: 0.7867 - val_loss: 1.8243 - val_accuracy: 0.6060\n",
      "Epoch 144/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6846 - accuracy: 0.7900\n",
      "Epoch 00144: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 58s 149ms/step - loss: 0.6846 - accuracy: 0.7900 - val_loss: 1.7788 - val_accuracy: 0.6089\n",
      "Epoch 145/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6923 - accuracy: 0.7857\n",
      "Epoch 00145: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 0.6923 - accuracy: 0.7857 - val_loss: 1.8313 - val_accuracy: 0.6052\n",
      "Epoch 146/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6878 - accuracy: 0.7884\n",
      "Epoch 00146: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 0.6878 - accuracy: 0.7884 - val_loss: 1.9305 - val_accuracy: 0.5911\n",
      "Epoch 147/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6919 - accuracy: 0.7873\n",
      "Epoch 00147: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 151ms/step - loss: 0.6919 - accuracy: 0.7873 - val_loss: 1.8460 - val_accuracy: 0.6099\n",
      "Epoch 148/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6839 - accuracy: 0.7886\n",
      "Epoch 00148: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 0.6839 - accuracy: 0.7886 - val_loss: 1.8322 - val_accuracy: 0.5978\n",
      "Epoch 149/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6800 - accuracy: 0.7883\n",
      "Epoch 00149: val_accuracy did not improve from 0.62000\n",
      "391/391 [==============================] - 59s 150ms/step - loss: 0.6800 - accuracy: 0.7883 - val_loss: 1.7041 - val_accuracy: 0.6153\n",
      "Epoch 150/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6774 - accuracy: 0.7904\n",
      "Epoch 00150: val_accuracy improved from 0.62000 to 0.63000, saving model to InceptionNet_BatchNorm_Adam.hdf5\n",
      "391/391 [==============================] - 58s 148ms/step - loss: 0.6774 - accuracy: 0.7904 - val_loss: 1.6902 - val_accuracy: 0.6300\n",
      "Epoch 151/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6808 - accuracy: 0.7893\n",
      "Epoch 00151: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 146ms/step - loss: 0.6808 - accuracy: 0.7893 - val_loss: 1.7844 - val_accuracy: 0.6019\n",
      "Epoch 152/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6789 - accuracy: 0.7904\n",
      "Epoch 00152: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 147ms/step - loss: 0.6789 - accuracy: 0.7904 - val_loss: 1.7007 - val_accuracy: 0.6215\n",
      "Epoch 153/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6668 - accuracy: 0.7950\n",
      "Epoch 00153: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 145ms/step - loss: 0.6668 - accuracy: 0.7950 - val_loss: 1.7941 - val_accuracy: 0.6044\n",
      "Epoch 154/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6683 - accuracy: 0.7912\n",
      "Epoch 00154: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 146ms/step - loss: 0.6683 - accuracy: 0.7912 - val_loss: 1.8136 - val_accuracy: 0.6114\n",
      "Epoch 155/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6645 - accuracy: 0.7949\n",
      "Epoch 00155: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 146ms/step - loss: 0.6645 - accuracy: 0.7949 - val_loss: 1.9296 - val_accuracy: 0.5957\n",
      "Epoch 156/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6727 - accuracy: 0.7923\n",
      "Epoch 00156: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 145ms/step - loss: 0.6727 - accuracy: 0.7923 - val_loss: 1.8824 - val_accuracy: 0.5988\n",
      "Epoch 157/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6564 - accuracy: 0.7964\n",
      "Epoch 00157: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 145ms/step - loss: 0.6564 - accuracy: 0.7964 - val_loss: 1.7725 - val_accuracy: 0.6157\n",
      "Epoch 158/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6629 - accuracy: 0.7953\n",
      "Epoch 00158: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 58s 148ms/step - loss: 0.6629 - accuracy: 0.7953 - val_loss: 1.9263 - val_accuracy: 0.5894\n",
      "Epoch 159/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6600 - accuracy: 0.7942\n",
      "Epoch 00159: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 58s 148ms/step - loss: 0.6600 - accuracy: 0.7942 - val_loss: 1.9850 - val_accuracy: 0.5946\n",
      "Epoch 160/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6620 - accuracy: 0.7977\n",
      "Epoch 00160: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 58s 147ms/step - loss: 0.6620 - accuracy: 0.7977 - val_loss: 1.7471 - val_accuracy: 0.6074\n",
      "Epoch 161/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6598 - accuracy: 0.7964\n",
      "Epoch 00161: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 146ms/step - loss: 0.6598 - accuracy: 0.7964 - val_loss: 1.8550 - val_accuracy: 0.6028\n",
      "Epoch 162/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6435 - accuracy: 0.8023\n",
      "Epoch 00162: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 146ms/step - loss: 0.6435 - accuracy: 0.8023 - val_loss: 1.7898 - val_accuracy: 0.6071\n",
      "Epoch 163/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6590 - accuracy: 0.7965\n",
      "Epoch 00163: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 145ms/step - loss: 0.6590 - accuracy: 0.7965 - val_loss: 2.0139 - val_accuracy: 0.5961\n",
      "Epoch 164/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6627 - accuracy: 0.7950\n",
      "Epoch 00164: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 145ms/step - loss: 0.6627 - accuracy: 0.7950 - val_loss: 2.0005 - val_accuracy: 0.6002\n",
      "Epoch 165/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6515 - accuracy: 0.7982\n",
      "Epoch 00165: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 145ms/step - loss: 0.6515 - accuracy: 0.7982 - val_loss: 1.8774 - val_accuracy: 0.6083\n",
      "Epoch 166/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6431 - accuracy: 0.7999\n",
      "Epoch 00166: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 146ms/step - loss: 0.6431 - accuracy: 0.7999 - val_loss: 1.7909 - val_accuracy: 0.6128\n",
      "Epoch 167/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6437 - accuracy: 0.8025\n",
      "Epoch 00167: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 145ms/step - loss: 0.6437 - accuracy: 0.8025 - val_loss: 1.9323 - val_accuracy: 0.6002\n",
      "Epoch 168/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6467 - accuracy: 0.8016\n",
      "Epoch 00168: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 56s 144ms/step - loss: 0.6467 - accuracy: 0.8016 - val_loss: 1.7734 - val_accuracy: 0.6209\n",
      "Epoch 169/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6407 - accuracy: 0.8010\n",
      "Epoch 00169: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 56s 144ms/step - loss: 0.6407 - accuracy: 0.8010 - val_loss: 1.8271 - val_accuracy: 0.6264\n",
      "Epoch 170/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6329 - accuracy: 0.8042\n",
      "Epoch 00170: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 56s 144ms/step - loss: 0.6329 - accuracy: 0.8042 - val_loss: 1.9826 - val_accuracy: 0.5988\n",
      "Epoch 171/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6407 - accuracy: 0.8009\n",
      "Epoch 00171: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 56s 144ms/step - loss: 0.6407 - accuracy: 0.8009 - val_loss: 1.7820 - val_accuracy: 0.6205\n",
      "Epoch 172/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6420 - accuracy: 0.8004\n",
      "Epoch 00172: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 147ms/step - loss: 0.6420 - accuracy: 0.8004 - val_loss: 1.8061 - val_accuracy: 0.6275\n",
      "Epoch 173/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6475 - accuracy: 0.7984\n",
      "Epoch 00173: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 146ms/step - loss: 0.6475 - accuracy: 0.7984 - val_loss: 1.7651 - val_accuracy: 0.6180\n",
      "Epoch 174/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6342 - accuracy: 0.8034\n",
      "Epoch 00174: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 145ms/step - loss: 0.6342 - accuracy: 0.8034 - val_loss: 1.7954 - val_accuracy: 0.6201\n",
      "Epoch 175/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6273 - accuracy: 0.8052\n",
      "Epoch 00175: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 145ms/step - loss: 0.6273 - accuracy: 0.8052 - val_loss: 1.8472 - val_accuracy: 0.6100\n",
      "Epoch 176/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6352 - accuracy: 0.8033\n",
      "Epoch 00176: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 145ms/step - loss: 0.6352 - accuracy: 0.8033 - val_loss: 1.7822 - val_accuracy: 0.6160\n",
      "Epoch 177/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6198 - accuracy: 0.8076\n",
      "Epoch 00177: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 146ms/step - loss: 0.6198 - accuracy: 0.8076 - val_loss: 1.7607 - val_accuracy: 0.6231\n",
      "Epoch 178/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6313 - accuracy: 0.8046\n",
      "Epoch 00178: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 145ms/step - loss: 0.6313 - accuracy: 0.8046 - val_loss: 1.7526 - val_accuracy: 0.6190\n",
      "Epoch 179/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6300 - accuracy: 0.8056\n",
      "Epoch 00179: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 145ms/step - loss: 0.6300 - accuracy: 0.8056 - val_loss: 1.7680 - val_accuracy: 0.6127\n",
      "Epoch 180/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6271 - accuracy: 0.8055\n",
      "Epoch 00180: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 145ms/step - loss: 0.6271 - accuracy: 0.8055 - val_loss: 1.8670 - val_accuracy: 0.5989\n",
      "Epoch 181/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6198 - accuracy: 0.8069\n",
      "Epoch 00181: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 145ms/step - loss: 0.6198 - accuracy: 0.8069 - val_loss: 1.8375 - val_accuracy: 0.6038\n",
      "Epoch 182/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6198 - accuracy: 0.8079\n",
      "Epoch 00182: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 146ms/step - loss: 0.6198 - accuracy: 0.8079 - val_loss: 1.9814 - val_accuracy: 0.5914\n",
      "Epoch 183/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6164 - accuracy: 0.8074\n",
      "Epoch 00183: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 145ms/step - loss: 0.6164 - accuracy: 0.8074 - val_loss: 1.9891 - val_accuracy: 0.5937\n",
      "Epoch 184/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6146 - accuracy: 0.8079\n",
      "Epoch 00184: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 145ms/step - loss: 0.6146 - accuracy: 0.8079 - val_loss: 1.9191 - val_accuracy: 0.6023\n",
      "Epoch 185/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6115 - accuracy: 0.8110\n",
      "Epoch 00185: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 145ms/step - loss: 0.6115 - accuracy: 0.8110 - val_loss: 1.8280 - val_accuracy: 0.6159\n",
      "Epoch 186/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6162 - accuracy: 0.8102\n",
      "Epoch 00186: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 145ms/step - loss: 0.6162 - accuracy: 0.8102 - val_loss: 1.8922 - val_accuracy: 0.6122\n",
      "Epoch 187/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6078 - accuracy: 0.8100\n",
      "Epoch 00187: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 145ms/step - loss: 0.6078 - accuracy: 0.8100 - val_loss: 1.7272 - val_accuracy: 0.6293\n",
      "Epoch 188/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6190 - accuracy: 0.8077\n",
      "Epoch 00188: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 146ms/step - loss: 0.6190 - accuracy: 0.8077 - val_loss: 1.9472 - val_accuracy: 0.5988\n",
      "Epoch 189/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6120 - accuracy: 0.8076\n",
      "Epoch 00189: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 145ms/step - loss: 0.6120 - accuracy: 0.8076 - val_loss: 2.0344 - val_accuracy: 0.5971\n",
      "Epoch 190/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6102 - accuracy: 0.8112\n",
      "Epoch 00190: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 146ms/step - loss: 0.6102 - accuracy: 0.8112 - val_loss: 1.9522 - val_accuracy: 0.6062\n",
      "Epoch 191/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6066 - accuracy: 0.8110\n",
      "Epoch 00191: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 145ms/step - loss: 0.6066 - accuracy: 0.8110 - val_loss: 1.8954 - val_accuracy: 0.6184\n",
      "Epoch 192/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6051 - accuracy: 0.8122\n",
      "Epoch 00192: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 145ms/step - loss: 0.6051 - accuracy: 0.8122 - val_loss: 2.0808 - val_accuracy: 0.5887\n",
      "Epoch 193/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6132 - accuracy: 0.8083\n",
      "Epoch 00193: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 146ms/step - loss: 0.6132 - accuracy: 0.8083 - val_loss: 1.9834 - val_accuracy: 0.5936\n",
      "Epoch 194/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6035 - accuracy: 0.8137\n",
      "Epoch 00194: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 145ms/step - loss: 0.6035 - accuracy: 0.8137 - val_loss: 1.8547 - val_accuracy: 0.6171\n",
      "Epoch 195/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6005 - accuracy: 0.8147\n",
      "Epoch 00195: val_accuracy did not improve from 0.63000\n",
      "391/391 [==============================] - 57s 146ms/step - loss: 0.6005 - accuracy: 0.8147 - val_loss: 1.8506 - val_accuracy: 0.6077\n",
      "Epoch 196/1000\n",
      "194/391 [=============>................] - ETA: 27s - loss: 0.5770 - accuracy: 0.8182"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9dee1feef7c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'InceptionNet_BatchNorm_Adam.hdf5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_weights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=10, verbose=1, mode='auto',restore_best_weights=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mhist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard,EarlyStopping\n",
    "\n",
    "(x_train, Y_train), (x_test, Y_test) = cifar100.load_data()\n",
    "#x_train = x_train.astype('float32') / 255\n",
    "#x_test = x_test.astype('float32') / 255\n",
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(Y_train,100)\n",
    "y_test = to_categorical(Y_test,100)\n",
    "\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "aug_data=ImageDataGenerator(\n",
    "        rotation_range=20,     #randomly rotate images in the range (20 degrees)\n",
    "        horizontal_flip=True,  #randomly flip images\n",
    "        width_shift_range=0.1, #randomly shift images horizontally (fraction of total width)\n",
    "        shear_range = 0.2,     #Shear Intensity (Shear angle in counter-clockwise direction in degrees)\n",
    "        height_shift_range=0.1,#randomly shift images vertically (fraction of total height)\n",
    "        zoom_range=0.2,        #Range for random zoom\n",
    "        brightness_range = (0.5, 1.5))   #Range for picking a brightness shift value\n",
    "aug_data.fit(x_train)\n",
    "\n",
    "adam=Adam(learning_rate=0.001,clipnorm=1,name='adam')\n",
    "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('InceptionNet_BatchNorm_Adam.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "#early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=10, verbose=1, mode='auto',restore_best_weights=True)\n",
    "hist=model.fit(aug_data.flow(x_train, y_train, batch_size=128),batch_size=128, epochs=1000, verbose=1, validation_data=(x_test, y_test),callbacks=[checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "zQXi18jK7vXL",
    "outputId": "18e4adf1-f159-4c8f-bbda-15474b1fa57c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prec: 0.639580139818279\n",
      "Recall: 0.609\n",
      "Accuracy: 0.609\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the model\n",
    "y_true = y_test.argmax(-1)\n",
    "y_pred = model.predict(x_test).argmax(-1)\n",
    "# generate confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
    "confusion_matrix(y_true, y_pred)\n",
    "# calculate prec, recall, accuracy\n",
    "print(\"Prec: \"+ str(precision_score(y_true, y_pred, average='weighted')))\n",
    "print(\"Recall: \"+ str(recall_score(y_true, y_pred, average='weighted')))\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kV_zVpmo55K"
   },
   "source": [
    "## Cell to Load Weights and Print Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "AWlxOOLmoC4v",
    "outputId": "6e13a813-da8b-4505-b73c-83704d862abf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prec: 0.6496617371313294\n",
      "Recall: 0.63\n",
      "Accuracy: 0.63\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import Tensor\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU,LeakyReLU, BatchNormalization,\\\n",
    "                                    Add, AveragePooling2D, Flatten, Dense, Dropout,ZeroPadding2D,MaxPool2D,concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "\n",
    "\n",
    "#Model Creation\n",
    "\n",
    "inputs = Input(shape=(32, 32, 3))\n",
    "X = Conv2D(32, (3,3), strides = (1,1),padding='same',activation=LeakyReLU())(inputs)\n",
    "X = Conv2D(32, (3,3), strides = (1,1),padding='same',activation=LeakyReLU())(X)\n",
    "X = Conv2D(32, (3,3), strides = (1,1),padding='same',activation=LeakyReLU())(X)\n",
    "X = BatchNormalization(axis = 3)(X)\n",
    "X = MaxPool2D((3,3), strides=(1,1), padding='same')(X)\n",
    "X = Conv2D(32, (3,3), strides = (1,1),padding='same',activation=LeakyReLU())(X)\n",
    "X = Conv2D(32, (3,3), strides = (1,1),padding='same',activation=LeakyReLU())(X)\n",
    "X = Conv2D(32, (3,3), strides = (1,1),padding='same',activation=LeakyReLU())(X)\n",
    "X = BatchNormalization(axis = 3)(X)\n",
    "\n",
    "#Inception1\n",
    "conv_1 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
    "conv_1 = Conv2D(32, (3,3), padding='same', activation=LeakyReLU())(conv_1)\n",
    "conv_1 = Conv2D(32, (3,3), padding='same', activation=LeakyReLU())(conv_1)\n",
    "conv_1 = BatchNormalization(axis = 3)(conv_1)\n",
    "\n",
    "conv_2 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
    "conv_2 = Conv2D(32, (3,3), padding='same', activation=LeakyReLU())(conv_2)\n",
    "conv_2 = BatchNormalization(axis = 3)(conv_2)\n",
    "\n",
    "bn = BatchNormalization(axis = 3)(X)\n",
    "conv_3 = MaxPool2D((3,3), strides=(1,1), padding='same')(bn)\n",
    "conv_3 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(conv_3)\n",
    "\n",
    "conv_4 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
    "\n",
    "X = concatenate([conv_1,conv_2,conv_3,conv_4], axis=3)\n",
    "\n",
    "\n",
    "#Inception2\n",
    "conv_1 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
    "conv_1 = Conv2D(32, (1,7), padding='same', activation=LeakyReLU())(conv_1)\n",
    "conv_1 = Conv2D(32, (7,1), padding='same', activation=LeakyReLU())(conv_1)\n",
    "conv_1 = Conv2D(32, (1,7), padding='same', activation=LeakyReLU())(conv_1)\n",
    "conv_1 = Conv2D(32, (7,1), padding='same', activation=LeakyReLU())(conv_1)\n",
    "conv_1 = BatchNormalization(axis = 3)(conv_1)\n",
    "\n",
    "conv_2 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
    "conv_2 = Conv2D(32, (1,7), padding='same', activation=LeakyReLU())(conv_2)\n",
    "conv_2 = Conv2D(32, (7,1), padding='same', activation=LeakyReLU())(conv_2)\n",
    "conv_2 = BatchNormalization(axis = 3)(conv_2)\n",
    "\n",
    "bn = BatchNormalization(axis = 3)(X)\n",
    "conv_3 = MaxPool2D((3,3), strides=(1,1), padding='same')(bn)\n",
    "conv_3 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(conv_3)\n",
    "\n",
    "conv_4 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
    "\n",
    "X = concatenate([conv_1,conv_2,conv_3,conv_4], axis=3)\n",
    "\n",
    "\n",
    "#Inception3\n",
    "conv_1 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
    "conv_1 = Conv2D(32, (3,3), padding='same', activation=LeakyReLU())(conv_1)\n",
    "conv_11 = Conv2D(32, (1,3), padding='same', activation=LeakyReLU())(conv_1)\n",
    "conv_11 = BatchNormalization(axis = 3)(conv_11)\n",
    "conv_12 = Conv2D(32, (3,1), padding='same', activation=LeakyReLU())(conv_1)\n",
    "conv_12 = BatchNormalization(axis = 3)(conv_12)\n",
    "\n",
    "conv_2 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
    "conv_21 = Conv2D(32, (1,3), padding='same', activation=LeakyReLU())(conv_2)\n",
    "conv_21 = BatchNormalization(axis = 3)(conv_21)\n",
    "conv_22 = Conv2D(32, (3,1), padding='same', activation=LeakyReLU())(conv_2)\n",
    "conv_22 = BatchNormalization(axis = 3)(conv_22)\n",
    "\n",
    "bn = BatchNormalization(axis = 3)(X)\n",
    "conv_3 = MaxPool2D((3,3), strides=(1,1), padding='same')(bn)\n",
    "conv_3 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(conv_3)\n",
    "\n",
    "conv_4 = Conv2D(32, (1,1), padding='same', activation=LeakyReLU())(X)\n",
    "\n",
    "X = concatenate([conv_11,conv_12,conv_21,conv_22,conv_3,conv_4], axis=3)\n",
    "\n",
    "\n",
    "X = Conv2D(32, 3, activation=LeakyReLU())(X)\n",
    "X = Conv2D(64, 3, activation=LeakyReLU())(X)\n",
    "X = AveragePooling2D(4)(X)\n",
    "X = Flatten()(X)\n",
    "X = Dense(512, activation=LeakyReLU())(X)\n",
    "outputs = Dense(100, activation='softmax')(X)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard,EarlyStopping\n",
    "\n",
    "(x_train, Y_train), (x_test, Y_test) = cifar100.load_data()\n",
    "#x_train = x_train.astype('float32') / 255\n",
    "#x_test = x_test.astype('float32') / 255\n",
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(Y_train,100)\n",
    "y_test = to_categorical(Y_test,100)\n",
    "\n",
    "model.load_weights('../weights/InceptionNet_BatchNorm_Adam_Final.hdf5')\n",
    "\n",
    "\n",
    "# Test the model\n",
    "y_true = y_test.argmax(-1)\n",
    "y_pred = model.predict(x_test).argmax(-1)\n",
    "# generate confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
    "confusion_matrix(y_true, y_pred)\n",
    "# calculate prec, recall, accuracy\n",
    "print(\"Prec: \"+ str(precision_score(y_true, y_pred, average='weighted')))\n",
    "print(\"Recall: \"+ str(recall_score(y_true, y_pred, average='weighted')))\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_true, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkf4trK1oKSJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Inception_Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
