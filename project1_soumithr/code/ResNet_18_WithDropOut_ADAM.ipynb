{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "_ZEGUtT_3_gR"
   },
   "outputs": [],
   "source": [
    "from tensorflow import Tensor\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU,LeakyReLU, BatchNormalization,\\\n",
    "                                    Add, AveragePooling2D, Flatten, Dense, Dropout,ZeroPadding2D\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "\n",
    "#Create Model\n",
    "def relu_bn(inputs: Tensor) -> Tensor:\n",
    "    relu = ReLU()(inputs)\n",
    "    bn = BatchNormalization()(relu)\n",
    "    return relu\n",
    "\n",
    "def residual_block(x: Tensor, downsample: bool, filters: int, kernel_size: int = 3) -> Tensor:\n",
    "    y = Conv2D(kernel_size=kernel_size,strides= (1 if not downsample else 2),filters=filters,padding=\"same\",kernel_initializer = glorot_uniform(seed=0))(x)\n",
    "    #y = Dropout(0 if not downsample else 0.5)(y)\n",
    "    y = relu_bn(y)\n",
    "    y = Conv2D(kernel_size=kernel_size,strides=1,filters=filters,padding=\"same\",kernel_initializer = glorot_uniform(seed=0))(y)\n",
    "\n",
    "    if downsample:\n",
    "        x = Conv2D(kernel_size=1,strides=2,filters=filters,padding=\"same\",kernel_initializer = glorot_uniform(seed=0))(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "\n",
    "    out = Add()([x, y])\n",
    "    out = relu_bn(out)\n",
    "    return out\n",
    "\n",
    "def create_res_net():\n",
    "    \n",
    "    inputs = Input(shape=(32, 32, 3))\n",
    "    num_filters = 64\n",
    "\n",
    "    t = BatchNormalization()(inputs)\n",
    "    t = Conv2D(kernel_size=3,strides=1,filters=num_filters,padding=\"same\",kernel_initializer = glorot_uniform(seed=0))(t)\n",
    "    t = relu_bn(t)\n",
    "    \n",
    "    num_blocks_list = [2,2, 2,2]\n",
    "    for i in range(len(num_blocks_list)):\n",
    "        num_blocks = num_blocks_list[i]\n",
    "        for j in range(num_blocks):\n",
    "            t = residual_block(t, downsample=(j==0 and i!=0), filters=num_filters)\n",
    "        num_filters *= 2\n",
    "    t = Dropout(0.5)(t)\n",
    "    t = AveragePooling2D(4)(t)\n",
    "    t = Flatten()(t)\n",
    "    t = Dense(512,activation='relu')(t)\n",
    "    outputs = Dense(100, activation='softmax')(t)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    adam=Adam(learning_rate=0.001,clipnorm=1,name='adam')\n",
    "    model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code snippet taken and modified from https://towardsdatascience.com/building-a-resnet-in-keras-e8f1322a49ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "qEWMvzRb4KxD",
    "outputId": "b5fed1eb-d5df-4cb9-bead-da87204e7be5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_21\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, 32, 32, 3)    12          input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_200 (Conv2D)             (None, 32, 32, 64)   1792        batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_170 (ReLU)                (None, 32, 32, 64)   0           conv2d_200[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_201 (Conv2D)             (None, 32, 32, 64)   36928       re_lu_170[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_171 (ReLU)                (None, 32, 32, 64)   0           conv2d_201[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_202 (Conv2D)             (None, 32, 32, 64)   36928       re_lu_171[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_80 (Add)                    (None, 32, 32, 64)   0           re_lu_170[0][0]                  \n",
      "                                                                 conv2d_202[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_172 (ReLU)                (None, 32, 32, 64)   0           add_80[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_203 (Conv2D)             (None, 32, 32, 64)   36928       re_lu_172[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_173 (ReLU)                (None, 32, 32, 64)   0           conv2d_203[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_204 (Conv2D)             (None, 32, 32, 64)   36928       re_lu_173[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_81 (Add)                    (None, 32, 32, 64)   0           re_lu_172[0][0]                  \n",
      "                                                                 conv2d_204[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_174 (ReLU)                (None, 32, 32, 64)   0           add_81[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_205 (Conv2D)             (None, 16, 16, 128)  73856       re_lu_174[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_207 (Conv2D)             (None, 16, 16, 128)  8320        re_lu_174[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_175 (ReLU)                (None, 16, 16, 128)  0           conv2d_205[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 16, 16, 128)  0           conv2d_207[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_206 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_175[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_82 (Add)                    (None, 16, 16, 128)  0           dropout_38[0][0]                 \n",
      "                                                                 conv2d_206[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_176 (ReLU)                (None, 16, 16, 128)  0           add_82[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_208 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_176[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_177 (ReLU)                (None, 16, 16, 128)  0           conv2d_208[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_209 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_177[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_83 (Add)                    (None, 16, 16, 128)  0           re_lu_176[0][0]                  \n",
      "                                                                 conv2d_209[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_178 (ReLU)                (None, 16, 16, 128)  0           add_83[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_210 (Conv2D)             (None, 8, 8, 256)    295168      re_lu_178[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_212 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_178[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_179 (ReLU)                (None, 8, 8, 256)    0           conv2d_210[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 8, 8, 256)    0           conv2d_212[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_211 (Conv2D)             (None, 8, 8, 256)    590080      re_lu_179[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_84 (Add)                    (None, 8, 8, 256)    0           dropout_39[0][0]                 \n",
      "                                                                 conv2d_211[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_180 (ReLU)                (None, 8, 8, 256)    0           add_84[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_213 (Conv2D)             (None, 8, 8, 256)    590080      re_lu_180[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_181 (ReLU)                (None, 8, 8, 256)    0           conv2d_213[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_214 (Conv2D)             (None, 8, 8, 256)    590080      re_lu_181[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_85 (Add)                    (None, 8, 8, 256)    0           re_lu_180[0][0]                  \n",
      "                                                                 conv2d_214[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_182 (ReLU)                (None, 8, 8, 256)    0           add_85[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_215 (Conv2D)             (None, 4, 4, 512)    1180160     re_lu_182[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_217 (Conv2D)             (None, 4, 4, 512)    131584      re_lu_182[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_183 (ReLU)                (None, 4, 4, 512)    0           conv2d_215[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 4, 4, 512)    0           conv2d_217[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_216 (Conv2D)             (None, 4, 4, 512)    2359808     re_lu_183[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_86 (Add)                    (None, 4, 4, 512)    0           dropout_40[0][0]                 \n",
      "                                                                 conv2d_216[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_184 (ReLU)                (None, 4, 4, 512)    0           add_86[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_218 (Conv2D)             (None, 4, 4, 512)    2359808     re_lu_184[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_185 (ReLU)                (None, 4, 4, 512)    0           conv2d_218[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_219 (Conv2D)             (None, 4, 4, 512)    2359808     re_lu_185[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_87 (Add)                    (None, 4, 4, 512)    0           re_lu_184[0][0]                  \n",
      "                                                                 conv2d_219[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_186 (ReLU)                (None, 4, 4, 512)    0           add_87[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 4, 4, 512)    0           re_lu_186[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_10 (AveragePo (None, 1, 1, 512)    0           dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 512)          0           average_pooling2d_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 512)          262656      flatten_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 100)          51300       dense_18[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 11,478,000\n",
      "Trainable params: 11,477,994\n",
      "Non-trainable params: 6\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 4.3890 - accuracy: 0.0256\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.04660, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 41s 106ms/step - loss: 4.3890 - accuracy: 0.0256 - val_loss: 4.2836 - val_accuracy: 0.0466\n",
      "Epoch 2/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.9883 - accuracy: 0.0730\n",
      "Epoch 00002: val_accuracy improved from 0.04660 to 0.09650, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 42s 106ms/step - loss: 3.9883 - accuracy: 0.0730 - val_loss: 3.8206 - val_accuracy: 0.0965\n",
      "Epoch 3/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.7124 - accuracy: 0.1197\n",
      "Epoch 00003: val_accuracy improved from 0.09650 to 0.13220, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 41s 105ms/step - loss: 3.7124 - accuracy: 0.1197 - val_loss: 3.6720 - val_accuracy: 0.1322\n",
      "Epoch 4/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.5140 - accuracy: 0.1535\n",
      "Epoch 00004: val_accuracy improved from 0.13220 to 0.20080, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 41s 105ms/step - loss: 3.5140 - accuracy: 0.1535 - val_loss: 3.3178 - val_accuracy: 0.2008\n",
      "Epoch 5/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.3368 - accuracy: 0.1865\n",
      "Epoch 00005: val_accuracy improved from 0.20080 to 0.20220, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 41s 105ms/step - loss: 3.3368 - accuracy: 0.1865 - val_loss: 3.2903 - val_accuracy: 0.2022\n",
      "Epoch 6/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.1893 - accuracy: 0.2124\n",
      "Epoch 00006: val_accuracy improved from 0.20220 to 0.24150, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 41s 105ms/step - loss: 3.1893 - accuracy: 0.2124 - val_loss: 3.0590 - val_accuracy: 0.2415\n",
      "Epoch 7/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.0609 - accuracy: 0.2379\n",
      "Epoch 00007: val_accuracy improved from 0.24150 to 0.28470, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 41s 105ms/step - loss: 3.0609 - accuracy: 0.2379 - val_loss: 2.8294 - val_accuracy: 0.2847\n",
      "Epoch 8/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.9300 - accuracy: 0.2646\n",
      "Epoch 00008: val_accuracy improved from 0.28470 to 0.30050, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 41s 105ms/step - loss: 2.9300 - accuracy: 0.2646 - val_loss: 2.7352 - val_accuracy: 0.3005\n",
      "Epoch 9/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.8263 - accuracy: 0.2831\n",
      "Epoch 00009: val_accuracy improved from 0.30050 to 0.30780, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 42s 107ms/step - loss: 2.8263 - accuracy: 0.2831 - val_loss: 2.7064 - val_accuracy: 0.3078\n",
      "Epoch 10/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.7360 - accuracy: 0.3006\n",
      "Epoch 00010: val_accuracy improved from 0.30780 to 0.33740, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 41s 105ms/step - loss: 2.7360 - accuracy: 0.3006 - val_loss: 2.6135 - val_accuracy: 0.3374\n",
      "Epoch 11/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.6453 - accuracy: 0.3210\n",
      "Epoch 00011: val_accuracy improved from 0.33740 to 0.35420, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 41s 105ms/step - loss: 2.6453 - accuracy: 0.3210 - val_loss: 2.4888 - val_accuracy: 0.3542\n",
      "Epoch 12/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.5642 - accuracy: 0.3367\n",
      "Epoch 00012: val_accuracy improved from 0.35420 to 0.37590, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 41s 106ms/step - loss: 2.5642 - accuracy: 0.3367 - val_loss: 2.3871 - val_accuracy: 0.3759\n",
      "Epoch 13/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.5028 - accuracy: 0.3490\n",
      "Epoch 00013: val_accuracy improved from 0.37590 to 0.38730, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 41s 106ms/step - loss: 2.5028 - accuracy: 0.3490 - val_loss: 2.3449 - val_accuracy: 0.3873\n",
      "Epoch 14/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.4441 - accuracy: 0.3618\n",
      "Epoch 00014: val_accuracy improved from 0.38730 to 0.38970, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 41s 105ms/step - loss: 2.4441 - accuracy: 0.3618 - val_loss: 2.3661 - val_accuracy: 0.3897\n",
      "Epoch 15/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.3742 - accuracy: 0.3769\n",
      "Epoch 00015: val_accuracy improved from 0.38970 to 0.41290, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 41s 106ms/step - loss: 2.3742 - accuracy: 0.3769 - val_loss: 2.2318 - val_accuracy: 0.4129\n",
      "Epoch 16/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.3400 - accuracy: 0.3853\n",
      "Epoch 00016: val_accuracy did not improve from 0.41290\n",
      "391/391 [==============================] - 41s 106ms/step - loss: 2.3400 - accuracy: 0.3853 - val_loss: 2.2720 - val_accuracy: 0.4088\n",
      "Epoch 17/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2859 - accuracy: 0.3942\n",
      "Epoch 00017: val_accuracy improved from 0.41290 to 0.41930, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 42s 107ms/step - loss: 2.2859 - accuracy: 0.3942 - val_loss: 2.2466 - val_accuracy: 0.4193\n",
      "Epoch 18/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2533 - accuracy: 0.4040\n",
      "Epoch 00018: val_accuracy improved from 0.41930 to 0.42990, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 41s 106ms/step - loss: 2.2533 - accuracy: 0.4040 - val_loss: 2.2065 - val_accuracy: 0.4299\n",
      "Epoch 19/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.2043 - accuracy: 0.4166\n",
      "Epoch 00019: val_accuracy improved from 0.42990 to 0.44390, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 42s 106ms/step - loss: 2.2043 - accuracy: 0.4166 - val_loss: 2.1174 - val_accuracy: 0.4439\n",
      "Epoch 20/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.1666 - accuracy: 0.4216\n",
      "Epoch 00020: val_accuracy did not improve from 0.44390\n",
      "391/391 [==============================] - 41s 106ms/step - loss: 2.1666 - accuracy: 0.4216 - val_loss: 2.1587 - val_accuracy: 0.4346\n",
      "Epoch 21/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.1367 - accuracy: 0.4272\n",
      "Epoch 00021: val_accuracy did not improve from 0.44390\n",
      "391/391 [==============================] - 41s 106ms/step - loss: 2.1367 - accuracy: 0.4272 - val_loss: 2.1562 - val_accuracy: 0.4364\n",
      "Epoch 22/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.1006 - accuracy: 0.4366\n",
      "Epoch 00022: val_accuracy did not improve from 0.44390\n",
      "391/391 [==============================] - 41s 106ms/step - loss: 2.1006 - accuracy: 0.4366 - val_loss: 2.1620 - val_accuracy: 0.4363\n",
      "Epoch 23/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.0679 - accuracy: 0.4437\n",
      "Epoch 00023: val_accuracy improved from 0.44390 to 0.46530, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 41s 106ms/step - loss: 2.0679 - accuracy: 0.4437 - val_loss: 2.0355 - val_accuracy: 0.4653\n",
      "Epoch 24/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.0484 - accuracy: 0.4472\n",
      "Epoch 00024: val_accuracy improved from 0.46530 to 0.47050, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 42s 107ms/step - loss: 2.0484 - accuracy: 0.4472 - val_loss: 2.0348 - val_accuracy: 0.4705\n",
      "Epoch 25/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 2.0184 - accuracy: 0.4543\n",
      "Epoch 00025: val_accuracy did not improve from 0.47050\n",
      "391/391 [==============================] - 41s 105ms/step - loss: 2.0184 - accuracy: 0.4543 - val_loss: 2.0240 - val_accuracy: 0.4656\n",
      "Epoch 26/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.9907 - accuracy: 0.4623\n",
      "Epoch 00026: val_accuracy did not improve from 0.47050\n",
      "391/391 [==============================] - 41s 105ms/step - loss: 1.9907 - accuracy: 0.4623 - val_loss: 2.0489 - val_accuracy: 0.4633\n",
      "Epoch 27/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.9718 - accuracy: 0.4668\n",
      "Epoch 00027: val_accuracy did not improve from 0.47050\n",
      "391/391 [==============================] - 41s 105ms/step - loss: 1.9718 - accuracy: 0.4668 - val_loss: 2.0528 - val_accuracy: 0.4686\n",
      "Epoch 28/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.9482 - accuracy: 0.4726\n",
      "Epoch 00028: val_accuracy did not improve from 0.47050\n",
      "391/391 [==============================] - 41s 105ms/step - loss: 1.9482 - accuracy: 0.4726 - val_loss: 2.0228 - val_accuracy: 0.4685\n",
      "Epoch 29/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.9233 - accuracy: 0.4774\n",
      "Epoch 00029: val_accuracy improved from 0.47050 to 0.48470, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 41s 105ms/step - loss: 1.9233 - accuracy: 0.4774 - val_loss: 1.9659 - val_accuracy: 0.4847\n",
      "Epoch 30/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.9126 - accuracy: 0.4788\n",
      "Epoch 00030: val_accuracy improved from 0.48470 to 0.48810, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 42s 107ms/step - loss: 1.9126 - accuracy: 0.4788 - val_loss: 1.9349 - val_accuracy: 0.4881\n",
      "Epoch 31/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.8911 - accuracy: 0.4836\n",
      "Epoch 00031: val_accuracy did not improve from 0.48810\n",
      "391/391 [==============================] - 41s 106ms/step - loss: 1.8911 - accuracy: 0.4836 - val_loss: 2.0741 - val_accuracy: 0.4663\n",
      "Epoch 32/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.8695 - accuracy: 0.4891\n",
      "Epoch 00032: val_accuracy improved from 0.48810 to 0.48900, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 43s 109ms/step - loss: 1.8695 - accuracy: 0.4891 - val_loss: 1.9440 - val_accuracy: 0.4890\n",
      "Epoch 33/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.8498 - accuracy: 0.4948\n",
      "Epoch 00033: val_accuracy improved from 0.48900 to 0.49580, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 42s 107ms/step - loss: 1.8498 - accuracy: 0.4948 - val_loss: 1.9173 - val_accuracy: 0.4958\n",
      "Epoch 34/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.8383 - accuracy: 0.4988\n",
      "Epoch 00034: val_accuracy did not improve from 0.49580\n",
      "391/391 [==============================] - 43s 109ms/step - loss: 1.8383 - accuracy: 0.4988 - val_loss: 1.9548 - val_accuracy: 0.4862\n",
      "Epoch 35/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.8173 - accuracy: 0.5005\n",
      "Epoch 00035: val_accuracy did not improve from 0.49580\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 1.8173 - accuracy: 0.5005 - val_loss: 1.9571 - val_accuracy: 0.4815\n",
      "Epoch 36/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.8040 - accuracy: 0.5041\n",
      "Epoch 00036: val_accuracy improved from 0.49580 to 0.49620, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 43s 111ms/step - loss: 1.8040 - accuracy: 0.5041 - val_loss: 1.9190 - val_accuracy: 0.4962\n",
      "Epoch 37/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.7900 - accuracy: 0.5064\n",
      "Epoch 00037: val_accuracy improved from 0.49620 to 0.49970, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 1.7900 - accuracy: 0.5064 - val_loss: 1.8868 - val_accuracy: 0.4997\n",
      "Epoch 38/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.7682 - accuracy: 0.5147\n",
      "Epoch 00038: val_accuracy did not improve from 0.49970\n",
      "391/391 [==============================] - 44s 111ms/step - loss: 1.7682 - accuracy: 0.5147 - val_loss: 1.9565 - val_accuracy: 0.4941\n",
      "Epoch 39/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.7662 - accuracy: 0.5184\n",
      "Epoch 00039: val_accuracy improved from 0.49970 to 0.50760, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 1.7662 - accuracy: 0.5184 - val_loss: 1.8733 - val_accuracy: 0.5076\n",
      "Epoch 40/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.7559 - accuracy: 0.5149\n",
      "Epoch 00040: val_accuracy did not improve from 0.50760\n",
      "391/391 [==============================] - 42s 108ms/step - loss: 1.7559 - accuracy: 0.5149 - val_loss: 1.8828 - val_accuracy: 0.5038\n",
      "Epoch 41/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.7296 - accuracy: 0.5218\n",
      "Epoch 00041: val_accuracy did not improve from 0.50760\n",
      "391/391 [==============================] - 42s 108ms/step - loss: 1.7296 - accuracy: 0.5218 - val_loss: 1.9338 - val_accuracy: 0.4963\n",
      "Epoch 42/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.7209 - accuracy: 0.5245\n",
      "Epoch 00042: val_accuracy did not improve from 0.50760\n",
      "391/391 [==============================] - 43s 109ms/step - loss: 1.7209 - accuracy: 0.5245 - val_loss: 1.9065 - val_accuracy: 0.5069\n",
      "Epoch 43/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.7084 - accuracy: 0.5272\n",
      "Epoch 00043: val_accuracy did not improve from 0.50760\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 1.7084 - accuracy: 0.5272 - val_loss: 1.9445 - val_accuracy: 0.4963\n",
      "Epoch 44/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.7145 - accuracy: 0.5266\n",
      "Epoch 00044: val_accuracy did not improve from 0.50760\n",
      "391/391 [==============================] - 43s 111ms/step - loss: 1.7145 - accuracy: 0.5266 - val_loss: 1.9512 - val_accuracy: 0.5022\n",
      "Epoch 45/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.6866 - accuracy: 0.5326\n",
      "Epoch 00045: val_accuracy did not improve from 0.50760\n",
      "391/391 [==============================] - 44s 111ms/step - loss: 1.6866 - accuracy: 0.5326 - val_loss: 1.9410 - val_accuracy: 0.4986\n",
      "Epoch 46/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.6828 - accuracy: 0.5324\n",
      "Epoch 00046: val_accuracy improved from 0.50760 to 0.52200, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 1.6828 - accuracy: 0.5324 - val_loss: 1.8327 - val_accuracy: 0.5220\n",
      "Epoch 47/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.6622 - accuracy: 0.5399\n",
      "Epoch 00047: val_accuracy did not improve from 0.52200\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 1.6622 - accuracy: 0.5399 - val_loss: 1.8461 - val_accuracy: 0.5197\n",
      "Epoch 48/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.6578 - accuracy: 0.5398\n",
      "Epoch 00048: val_accuracy did not improve from 0.52200\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 1.6578 - accuracy: 0.5398 - val_loss: 1.8876 - val_accuracy: 0.5109\n",
      "Epoch 49/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.6460 - accuracy: 0.5416\n",
      "Epoch 00049: val_accuracy did not improve from 0.52200\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 1.6460 - accuracy: 0.5416 - val_loss: 1.8791 - val_accuracy: 0.5119\n",
      "Epoch 50/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.6389 - accuracy: 0.5421\n",
      "Epoch 00050: val_accuracy did not improve from 0.52200\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 1.6389 - accuracy: 0.5421 - val_loss: 1.8530 - val_accuracy: 0.5140\n",
      "Epoch 51/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.6307 - accuracy: 0.5460\n",
      "Epoch 00051: val_accuracy improved from 0.52200 to 0.52300, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 43s 111ms/step - loss: 1.6307 - accuracy: 0.5460 - val_loss: 1.7993 - val_accuracy: 0.5230\n",
      "Epoch 52/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.6291 - accuracy: 0.5475\n",
      "Epoch 00052: val_accuracy improved from 0.52300 to 0.52420, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 1.6291 - accuracy: 0.5475 - val_loss: 1.8051 - val_accuracy: 0.5242\n",
      "Epoch 53/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.6123 - accuracy: 0.5511\n",
      "Epoch 00053: val_accuracy did not improve from 0.52420\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 1.6123 - accuracy: 0.5511 - val_loss: 1.8408 - val_accuracy: 0.5209\n",
      "Epoch 54/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.6005 - accuracy: 0.5510\n",
      "Epoch 00054: val_accuracy did not improve from 0.52420\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 1.6005 - accuracy: 0.5510 - val_loss: 2.0281 - val_accuracy: 0.4879\n",
      "Epoch 55/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5955 - accuracy: 0.5593\n",
      "Epoch 00055: val_accuracy did not improve from 0.52420\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 1.5955 - accuracy: 0.5593 - val_loss: 1.9095 - val_accuracy: 0.5085\n",
      "Epoch 56/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5880 - accuracy: 0.5569\n",
      "Epoch 00056: val_accuracy improved from 0.52420 to 0.52800, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 45s 114ms/step - loss: 1.5880 - accuracy: 0.5569 - val_loss: 1.8345 - val_accuracy: 0.5280\n",
      "Epoch 57/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5684 - accuracy: 0.5616\n",
      "Epoch 00057: val_accuracy did not improve from 0.52800\n",
      "391/391 [==============================] - 44s 114ms/step - loss: 1.5684 - accuracy: 0.5616 - val_loss: 1.8139 - val_accuracy: 0.5245\n",
      "Epoch 58/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5734 - accuracy: 0.5580\n",
      "Epoch 00058: val_accuracy did not improve from 0.52800\n",
      "391/391 [==============================] - 45s 114ms/step - loss: 1.5734 - accuracy: 0.5580 - val_loss: 1.8647 - val_accuracy: 0.5194\n",
      "Epoch 59/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5642 - accuracy: 0.5646\n",
      "Epoch 00059: val_accuracy did not improve from 0.52800\n",
      "391/391 [==============================] - 45s 114ms/step - loss: 1.5642 - accuracy: 0.5646 - val_loss: 1.9419 - val_accuracy: 0.5065\n",
      "Epoch 60/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5563 - accuracy: 0.5659\n",
      "Epoch 00060: val_accuracy did not improve from 0.52800\n",
      "391/391 [==============================] - 45s 114ms/step - loss: 1.5563 - accuracy: 0.5659 - val_loss: 1.8511 - val_accuracy: 0.5205\n",
      "Epoch 61/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5546 - accuracy: 0.5650\n",
      "Epoch 00061: val_accuracy did not improve from 0.52800\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 1.5546 - accuracy: 0.5650 - val_loss: 1.8167 - val_accuracy: 0.5265\n",
      "Epoch 62/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5431 - accuracy: 0.5707\n",
      "Epoch 00062: val_accuracy did not improve from 0.52800\n",
      "391/391 [==============================] - 45s 114ms/step - loss: 1.5431 - accuracy: 0.5707 - val_loss: 1.8739 - val_accuracy: 0.5211\n",
      "Epoch 63/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5434 - accuracy: 0.5673\n",
      "Epoch 00063: val_accuracy did not improve from 0.52800\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 1.5434 - accuracy: 0.5673 - val_loss: 1.8940 - val_accuracy: 0.5129\n",
      "Epoch 64/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5399 - accuracy: 0.5664\n",
      "Epoch 00064: val_accuracy improved from 0.52800 to 0.53460, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 1.5399 - accuracy: 0.5664 - val_loss: 1.7766 - val_accuracy: 0.5346\n",
      "Epoch 65/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5188 - accuracy: 0.5757\n",
      "Epoch 00065: val_accuracy did not improve from 0.53460\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 1.5188 - accuracy: 0.5757 - val_loss: 1.8960 - val_accuracy: 0.5182\n",
      "Epoch 66/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5179 - accuracy: 0.5722\n",
      "Epoch 00066: val_accuracy did not improve from 0.53460\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 1.5179 - accuracy: 0.5722 - val_loss: 1.7991 - val_accuracy: 0.5299\n",
      "Epoch 67/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5178 - accuracy: 0.5747\n",
      "Epoch 00067: val_accuracy improved from 0.53460 to 0.55100, saving model to ResNet_DropOUT_Adam.hdf5\n",
      "391/391 [==============================] - 45s 114ms/step - loss: 1.5178 - accuracy: 0.5747 - val_loss: 1.7161 - val_accuracy: 0.5510\n",
      "Epoch 68/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5055 - accuracy: 0.5784\n",
      "Epoch 00068: val_accuracy did not improve from 0.55100\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 1.5055 - accuracy: 0.5784 - val_loss: 1.8772 - val_accuracy: 0.5209\n",
      "Epoch 69/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5162 - accuracy: 0.5738\n",
      "Epoch 00069: val_accuracy did not improve from 0.55100\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 1.5162 - accuracy: 0.5738 - val_loss: 1.8329 - val_accuracy: 0.5262\n",
      "Epoch 70/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.4987 - accuracy: 0.5779\n",
      "Epoch 00070: val_accuracy did not improve from 0.55100\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 1.4987 - accuracy: 0.5779 - val_loss: 1.7913 - val_accuracy: 0.5336\n",
      "Epoch 71/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.4941 - accuracy: 0.5801\n",
      "Epoch 00071: val_accuracy did not improve from 0.55100\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 1.4941 - accuracy: 0.5801 - val_loss: 1.9239 - val_accuracy: 0.5072\n",
      "Epoch 72/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.4951 - accuracy: 0.5768\n",
      "Epoch 00072: val_accuracy did not improve from 0.55100\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 1.4951 - accuracy: 0.5768 - val_loss: 1.9127 - val_accuracy: 0.5183\n",
      "Epoch 73/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.4815 - accuracy: 0.5843\n",
      "Epoch 00073: val_accuracy did not improve from 0.55100\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 1.4815 - accuracy: 0.5843 - val_loss: 1.9604 - val_accuracy: 0.5119\n",
      "Epoch 74/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.4897 - accuracy: 0.5804\n",
      "Epoch 00074: val_accuracy did not improve from 0.55100\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 1.4897 - accuracy: 0.5804 - val_loss: 1.8712 - val_accuracy: 0.5234\n",
      "Epoch 75/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.4823 - accuracy: 0.5869\n",
      "Epoch 00075: val_accuracy did not improve from 0.55100\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 1.4823 - accuracy: 0.5869 - val_loss: 1.7796 - val_accuracy: 0.5414\n",
      "Epoch 76/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.4887 - accuracy: 0.5812\n",
      "Epoch 00076: val_accuracy did not improve from 0.55100\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 1.4887 - accuracy: 0.5812 - val_loss: 1.8986 - val_accuracy: 0.5224\n",
      "Epoch 77/1000\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.4662 - accuracy: 0.5863Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.55100\n",
      "391/391 [==============================] - 45s 114ms/step - loss: 1.4662 - accuracy: 0.5863 - val_loss: 1.7825 - val_accuracy: 0.5385\n",
      "Epoch 00077: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard,EarlyStopping\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "(x_train, Y_train), (x_test, Y_test) = cifar100.load_data()\n",
    "#x_train = x_train.astype('float32') / 255\n",
    "#x_test = x_test.astype('float32') / 255\n",
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(Y_train,100)\n",
    "y_test = to_categorical(Y_test,100)\n",
    "\n",
    "model = create_res_net() # or create_plain_net()\n",
    "model.summary()\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "aug_data=ImageDataGenerator(rotation_range=20,horizontal_flip=True,width_shift_range=0.1,shear_range = 0.2,height_shift_range=0.1,zoom_range=0.2,brightness_range = (0.5, 1.5))\n",
    "aug_data.fit(x_train)\n",
    "\n",
    "# save model after each epoch\n",
    "checkpoint = ModelCheckpoint('ResNet_DropOUT_Adam.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=10, verbose=1, mode='auto',restore_best_weights=True)\n",
    "hist=model.fit(aug_data.flow(x_train, y_train, batch_size=128),batch_size=128, epochs=1000, verbose=1, validation_data=(x_test, y_test),callbacks=[early,checkpoint])\n",
    "\n",
    "#model.fit(x=x_train,y=y_train,epochs=20,verbose=1,validation_data=(x_test, y_test),batch_size=128,callbacks=[checkpoint, tensorboard_callback,early]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "zYO8YFG94bPb",
    "outputId": "d2e71a8f-e360-4bfd-dcca-39526c05fa02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prec: 0.5591015522303211\n",
      "Recall: 0.551\n",
      "Accuracy: 0.551\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the model\n",
    "y_true = y_test.argmax(-1)\n",
    "y_pred = model.predict(x_test).argmax(-1)\n",
    "# generate confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
    "confusion_matrix(y_true, y_pred)\n",
    "# calculate prec, recall, accuracy\n",
    "print(\"Prec: \"+ str(precision_score(y_true, y_pred, average='weighted')))\n",
    "print(\"Recall: \"+ str(recall_score(y_true, y_pred, average='weighted')))\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "-_HVB_0b9rW8",
    "outputId": "6e500013-293f-4d29-c993-4096b7b2c8b0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist.history[\"accuracy\"])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title(\"model accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Accuracy\",\"Validation Accuracy\",\"loss\",\"Validation Loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell to Load Weights and Print Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BKz-D7jL9tHt"
   },
   "outputs": [],
   "source": [
    "from tensorflow import Tensor\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU,LeakyReLU, BatchNormalization,\\\n",
    "                                    Add, AveragePooling2D, Flatten, Dense, Dropout,ZeroPadding2D\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "\n",
    "def relu_bn(inputs: Tensor) -> Tensor:\n",
    "    relu = ReLU()(inputs)\n",
    "    bn = BatchNormalization()(relu)\n",
    "    return relu\n",
    "\n",
    "def residual_block(x: Tensor, downsample: bool, filters: int, kernel_size: int = 3) -> Tensor:\n",
    "    y = Conv2D(kernel_size=kernel_size,strides= (1 if not downsample else 2),filters=filters,padding=\"same\",kernel_initializer = glorot_uniform(seed=0))(x)\n",
    "    #y = Dropout(0 if not downsample else 0.5)(y)\n",
    "    y = relu_bn(y)\n",
    "    y = Conv2D(kernel_size=kernel_size,strides=1,filters=filters,padding=\"same\",kernel_initializer = glorot_uniform(seed=0))(y)\n",
    "\n",
    "    if downsample:\n",
    "        x = Conv2D(kernel_size=1,strides=2,filters=filters,padding=\"same\",kernel_initializer = glorot_uniform(seed=0))(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "\n",
    "    out = Add()([x, y])\n",
    "    out = relu_bn(out)\n",
    "    return out\n",
    "\n",
    "def create_res_net():\n",
    "    \n",
    "    inputs = Input(shape=(32, 32, 3))\n",
    "    num_filters = 64\n",
    "\n",
    "    t = BatchNormalization()(inputs)\n",
    "    t = Conv2D(kernel_size=3,strides=1,filters=num_filters,padding=\"same\",kernel_initializer = glorot_uniform(seed=0))(t)\n",
    "    t = relu_bn(t)\n",
    "    \n",
    "    num_blocks_list = [2,2, 2,2]\n",
    "    for i in range(len(num_blocks_list)):\n",
    "        num_blocks = num_blocks_list[i]\n",
    "        for j in range(num_blocks):\n",
    "            t = residual_block(t, downsample=(j==0 and i!=0), filters=num_filters)\n",
    "        num_filters *= 2\n",
    "    t = Dropout(0.5)(t)\n",
    "    t = AveragePooling2D(4)(t)\n",
    "    t = Flatten()(t)\n",
    "    t = Dense(512,activation='relu')(t)\n",
    "    outputs = Dense(100, activation='softmax')(t)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    adam=Adam(learning_rate=0.001,clipnorm=1,name='adam')\n",
    "    model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard,EarlyStopping\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "(x_train, Y_train), (x_test, Y_test) = cifar100.load_data()\n",
    "#x_train = x_train.astype('float32') / 255\n",
    "#x_test = x_test.astype('float32') / 255\n",
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(Y_train,100)\n",
    "y_test = to_categorical(Y_test,100)\n",
    "\n",
    "model = create_res_net() # or create_plain_net()\n",
    "\n",
    "model.load_weights('../weights/ResNet_DropOUT_Adam.hdf5')\n",
    "\n",
    "# Test the model\n",
    "y_true = y_test.argmax(-1)\n",
    "y_pred = model.predict(x_test).argmax(-1)\n",
    "# generate confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
    "confusion_matrix(y_true, y_pred)\n",
    "# calculate prec, recall, accuracy\n",
    "print(\"Prec: \"+ str(precision_score(y_true, y_pred, average='weighted')))\n",
    "print(\"Recall: \"+ str(recall_score(y_true, y_pred, average='weighted')))\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_true, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ResNet_DropOut_Adam.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
